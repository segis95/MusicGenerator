{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACTIVE TEXT SUMMARIZATION: \"REFRESH\" MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This document gives the interactive access to functionality of the project. One can put the text files into an input directory \n",
    "- Please follow the instruction and ensure that your files are UTF-8 encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLEASE WAIT UNTIL THE CELL BELOW STOPS RUNNING...PREPARING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "The raw code for this IPython notebook is by default hidden for easier reading.To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "import my_flags\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "tf.app.flags.DEFINE_string(\"f\", \"\", \"kernel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42e40ec36d94de0ac1f6175f4aa562f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='my_input_directory', description='Input dir:', placeholder='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88dc1ebab764d83bdf341c66273e937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', disabled=True, placeholder='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe836951af2d4155902a2673c278a212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='warning', description='Reset input files...', icon='check', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9cbdb5b36b546af914928bb5af978ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Validate the split', icon='check', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the embedding...\n",
      "Reading pretrained word embeddings file: data/1-billion-word-language-modeling-benchmark-r13output.word2vec.vec\n",
      "0 ...\n",
      "100000 ...\n",
      "200000 ...\n",
      "300000 ...\n",
      "400000 ...\n",
      "500000 ...\n",
      "Read pretrained embeddings: (559183, 200)\n",
      "Size of vocab: 559185 (_PAD:0, _UNK:1)\n",
      "Writing vocab file: trainingdir/vocab.txt\n",
      "Starting summarization...\n",
      "Prepare vocab dict and read pretrained word embeddings ...\n",
      "Reading pretrained word embeddings file: data/1-billion-word-language-modeling-benchmark-r13output.word2vec.vec\n",
      "0 ...\n",
      "100000 ...\n",
      "200000 ...\n",
      "300000 ...\n",
      "400000 ...\n",
      "500000 ...\n",
      "Read pretrained embeddings: (559183, 200)\n",
      "Size of vocab: 559185 (_PAD:0, _UNK:1)\n",
      "Writing vocab file: trainingdir/vocab.txt\n",
      "Prepare test data ...\n",
      "Data file prefix (.doc, .title, .image, .label.multipleoracle): data/mypreprocdir/bloomberg.test\n",
      "Data sizes: 2 \n",
      "Reading data (no padding to save memory) ...\n",
      "0 ...\n",
      "Writing data files with prefix (.filename, .doc, .title, .image, .label, .weight, .rewards): trainingdir/bloomberg.test\n",
      "WARNING:tensorflow:From C:\\Users\\Charles\\Documents\\Sergey\\Text summarization project\\my_model.py:78: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "Reading model parameters from trainingdir/model.ckpt.epoch-15\n",
      "INFO:tensorflow:Restoring parameters from trainingdir/model.ckpt.epoch-15\n",
      "Model loaded.\n",
      "Initialize word embedding vocabulary with pretrained embeddings ...\n",
      "Performance on the test data:\n",
      "['blmbrg-try1', 'blmbrg-try2']\n",
      "blmbrg-try1\n",
      "['blmbrg-try1', 'blmbrg-try2']\n",
      "blmbrg-try2\n",
      "Writing predictions and final summaries ...\n",
      "Writing summaries...\n",
      "Loading the embedding...\n",
      "Reading pretrained word embeddings file: data/1-billion-word-language-modeling-benchmark-r13output.word2vec.vec\n",
      "0 ...\n",
      "100000 ...\n",
      "200000 ...\n",
      "300000 ...\n",
      "400000 ...\n",
      "500000 ...\n",
      "Read pretrained embeddings: (559183, 200)\n",
      "Size of vocab: 559185 (_PAD:0, _UNK:1)\n",
      "Writing vocab file: trainingdir/vocab.txt\n",
      "Starting summarization...\n",
      "Prepare vocab dict and read pretrained word embeddings ...\n",
      "Reading pretrained word embeddings file: data/1-billion-word-language-modeling-benchmark-r13output.word2vec.vec\n",
      "0 ...\n",
      "100000 ...\n",
      "200000 ...\n",
      "300000 ...\n",
      "400000 ...\n",
      "500000 ...\n",
      "Read pretrained embeddings: (559183, 200)\n",
      "Size of vocab: 559185 (_PAD:0, _UNK:1)\n",
      "Writing vocab file: trainingdir/vocab.txt\n",
      "Prepare test data ...\n",
      "Data file prefix (.doc, .title, .image, .label.multipleoracle): data/mypreprocdir/bloomberg.test\n",
      "Data sizes: 2 \n",
      "Reading data (no padding to save memory) ...\n",
      "0 ...\n",
      "Writing data files with prefix (.filename, .doc, .title, .image, .label, .weight, .rewards): trainingdir/bloomberg.test\n",
      "Reading model parameters from trainingdir/model.ckpt.epoch-15\n",
      "INFO:tensorflow:Restoring parameters from trainingdir/model.ckpt.epoch-15\n",
      "Model loaded.\n",
      "Initialize word embedding vocabulary with pretrained embeddings ...\n",
      "Performance on the test data:\n",
      "['blmbrg-try1', 'blmbrg-try2']\n",
      "blmbrg-try1\n",
      "['blmbrg-try1', 'blmbrg-try2']\n",
      "blmbrg-try2\n",
      "Writing predictions and final summaries ...\n",
      "Writing summaries...\n",
      "Loading the embedding...\n",
      "Reading pretrained word embeddings file: data/1-billion-word-language-modeling-benchmark-r13output.word2vec.vec\n",
      "0 ...\n",
      "100000 ...\n",
      "200000 ...\n",
      "300000 ...\n",
      "400000 ...\n",
      "500000 ...\n",
      "Read pretrained embeddings: (559183, 200)\n",
      "Size of vocab: 559185 (_PAD:0, _UNK:1)\n",
      "Writing vocab file: trainingdir/vocab.txt\n",
      "Starting summarization...\n",
      "Prepare vocab dict and read pretrained word embeddings ...\n",
      "Reading pretrained word embeddings file: data/1-billion-word-language-modeling-benchmark-r13output.word2vec.vec\n",
      "0 ...\n",
      "100000 ...\n",
      "200000 ...\n",
      "300000 ...\n",
      "400000 ...\n",
      "500000 ...\n",
      "Read pretrained embeddings: (559183, 200)\n",
      "Size of vocab: 559185 (_PAD:0, _UNK:1)\n",
      "Writing vocab file: trainingdir/vocab.txt\n",
      "Prepare test data ...\n",
      "Data file prefix (.doc, .title, .image, .label.multipleoracle): data/mypreprocdir/bloomberg.test\n",
      "Data sizes: 12 \n",
      "Reading data (no padding to save memory) ...\n",
      "0 ...\n",
      "Writing data files with prefix (.filename, .doc, .title, .image, .label, .weight, .rewards): trainingdir/bloomberg.test\n",
      "Reading model parameters from trainingdir/model.ckpt.epoch-15\n",
      "INFO:tensorflow:Restoring parameters from trainingdir/model.ckpt.epoch-15\n",
      "Model loaded.\n",
      "Initialize word embedding vocabulary with pretrained embeddings ...\n",
      "Performance on the test data:\n",
      "['blmbrg-Alibaba', 'blmbrg-BirdRaces', 'blmbrg-CanadaBuysKinder', 'blmbrg-ChinaEconomyBattling', 'blmbrg-IndonesiaRadeHike', 'blmbrg-JABCoffeeAndFood', 'blmbrg-KKRBuysBain', 'blmbrg-SexualViolence', 'blmbrg-SpainNominates', 'blmbrg-TrumpRatchets', 'blmbrg-try1', 'blmbrg-try2']\n",
      "blmbrg-Alibaba\n",
      "['blmbrg-Alibaba', 'blmbrg-BirdRaces', 'blmbrg-CanadaBuysKinder', 'blmbrg-ChinaEconomyBattling', 'blmbrg-IndonesiaRadeHike', 'blmbrg-JABCoffeeAndFood', 'blmbrg-KKRBuysBain', 'blmbrg-SexualViolence', 'blmbrg-SpainNominates', 'blmbrg-TrumpRatchets', 'blmbrg-try1', 'blmbrg-try2']\n",
      "blmbrg-BirdRaces\n",
      "['blmbrg-Alibaba', 'blmbrg-BirdRaces', 'blmbrg-CanadaBuysKinder', 'blmbrg-ChinaEconomyBattling', 'blmbrg-IndonesiaRadeHike', 'blmbrg-JABCoffeeAndFood', 'blmbrg-KKRBuysBain', 'blmbrg-SexualViolence', 'blmbrg-SpainNominates', 'blmbrg-TrumpRatchets', 'blmbrg-try1', 'blmbrg-try2']\n",
      "blmbrg-CanadaBuysKinder\n",
      "['blmbrg-Alibaba', 'blmbrg-BirdRaces', 'blmbrg-CanadaBuysKinder', 'blmbrg-ChinaEconomyBattling', 'blmbrg-IndonesiaRadeHike', 'blmbrg-JABCoffeeAndFood', 'blmbrg-KKRBuysBain', 'blmbrg-SexualViolence', 'blmbrg-SpainNominates', 'blmbrg-TrumpRatchets', 'blmbrg-try1', 'blmbrg-try2']\n",
      "blmbrg-ChinaEconomyBattling\n",
      "['blmbrg-Alibaba', 'blmbrg-BirdRaces', 'blmbrg-CanadaBuysKinder', 'blmbrg-ChinaEconomyBattling', 'blmbrg-IndonesiaRadeHike', 'blmbrg-JABCoffeeAndFood', 'blmbrg-KKRBuysBain', 'blmbrg-SexualViolence', 'blmbrg-SpainNominates', 'blmbrg-TrumpRatchets', 'blmbrg-try1', 'blmbrg-try2']\n",
      "blmbrg-IndonesiaRadeHike\n",
      "['blmbrg-Alibaba', 'blmbrg-BirdRaces', 'blmbrg-CanadaBuysKinder', 'blmbrg-ChinaEconomyBattling', 'blmbrg-IndonesiaRadeHike', 'blmbrg-JABCoffeeAndFood', 'blmbrg-KKRBuysBain', 'blmbrg-SexualViolence', 'blmbrg-SpainNominates', 'blmbrg-TrumpRatchets', 'blmbrg-try1', 'blmbrg-try2']\n",
      "blmbrg-JABCoffeeAndFood\n",
      "['blmbrg-Alibaba', 'blmbrg-BirdRaces', 'blmbrg-CanadaBuysKinder', 'blmbrg-ChinaEconomyBattling', 'blmbrg-IndonesiaRadeHike', 'blmbrg-JABCoffeeAndFood', 'blmbrg-KKRBuysBain', 'blmbrg-SexualViolence', 'blmbrg-SpainNominates', 'blmbrg-TrumpRatchets', 'blmbrg-try1', 'blmbrg-try2']\n",
      "blmbrg-KKRBuysBain\n",
      "['blmbrg-Alibaba', 'blmbrg-BirdRaces', 'blmbrg-CanadaBuysKinder', 'blmbrg-ChinaEconomyBattling', 'blmbrg-IndonesiaRadeHike', 'blmbrg-JABCoffeeAndFood', 'blmbrg-KKRBuysBain', 'blmbrg-SexualViolence', 'blmbrg-SpainNominates', 'blmbrg-TrumpRatchets', 'blmbrg-try1', 'blmbrg-try2']\n",
      "blmbrg-SexualViolence\n",
      "['blmbrg-Alibaba', 'blmbrg-BirdRaces', 'blmbrg-CanadaBuysKinder', 'blmbrg-ChinaEconomyBattling', 'blmbrg-IndonesiaRadeHike', 'blmbrg-JABCoffeeAndFood', 'blmbrg-KKRBuysBain', 'blmbrg-SexualViolence', 'blmbrg-SpainNominates', 'blmbrg-TrumpRatchets', 'blmbrg-try1', 'blmbrg-try2']\n",
      "blmbrg-SpainNominates\n",
      "['blmbrg-Alibaba', 'blmbrg-BirdRaces', 'blmbrg-CanadaBuysKinder', 'blmbrg-ChinaEconomyBattling', 'blmbrg-IndonesiaRadeHike', 'blmbrg-JABCoffeeAndFood', 'blmbrg-KKRBuysBain', 'blmbrg-SexualViolence', 'blmbrg-SpainNominates', 'blmbrg-TrumpRatchets', 'blmbrg-try1', 'blmbrg-try2']\n",
      "blmbrg-TrumpRatchets\n",
      "['blmbrg-Alibaba', 'blmbrg-BirdRaces', 'blmbrg-CanadaBuysKinder', 'blmbrg-ChinaEconomyBattling', 'blmbrg-IndonesiaRadeHike', 'blmbrg-JABCoffeeAndFood', 'blmbrg-KKRBuysBain', 'blmbrg-SexualViolence', 'blmbrg-SpainNominates', 'blmbrg-TrumpRatchets', 'blmbrg-try1', 'blmbrg-try2']\n",
      "blmbrg-try1\n",
      "['blmbrg-Alibaba', 'blmbrg-BirdRaces', 'blmbrg-CanadaBuysKinder', 'blmbrg-ChinaEconomyBattling', 'blmbrg-IndonesiaRadeHike', 'blmbrg-JABCoffeeAndFood', 'blmbrg-KKRBuysBain', 'blmbrg-SexualViolence', 'blmbrg-SpainNominates', 'blmbrg-TrumpRatchets', 'blmbrg-try1', 'blmbrg-try2']\n",
      "blmbrg-try2\n",
      "Writing predictions and final summaries ...\n",
      "Writing summaries...\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from functools import reduce\n",
    "from data_utils import DataProcessor\n",
    "import tensorflow as tf\n",
    "\n",
    "#tokenizer.tokenize('Fig. 2 shows a U.S.A. map.')\n",
    "#tf.app.flags.DEFINE_string(\"f\", \"\", \"kernel\")\n",
    "\n",
    "def split_text(text):\n",
    "    punkt_param = PunktParameters()\n",
    "    abbreviation = ['u.s.a', 'fig', 'ph.d', 'ltd', 'u.s', 'corp', 'plc', 'co', 'inc',\\\n",
    "               'dr', 'ms', 'mr', 'mrs', 'jan', 'feb', 'mar', 'jun', 'jul', 'aug', 'sept', 'oct', 'nov', 'dec',\\\n",
    "               'a.m', 'p.m', 'e.g', 'i.e', 'u.k']\n",
    "\n",
    "    punkt_param.abbrev_types = set(abbreviation)\n",
    "    tokenizer = PunktSentenceTokenizer(punkt_param)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\r', ' ')\n",
    "    spl = tokenizer.tokenize(text)\n",
    "    out = []\n",
    "    i = 0\n",
    "    for s in spl:\n",
    "        i+=1\n",
    "        \n",
    "        if s[-1] == '.':\n",
    "            s = s[:-1] + ' .'\n",
    "        \n",
    "        s = s.replace('“', '\"')\n",
    "        s = s.replace('”', '\"')\n",
    "        s = s.replace('.\"', ' . \" ')\n",
    "        s = s.replace('\"', ' \" ')\n",
    "        s = s.replace(',', ' , ')\n",
    "        s = s.replace(':', ' : ')\n",
    "        s = s.replace(';', ' ; ')\n",
    "        s = s.replace('’', \"'\")\n",
    "        s = s.replace(\"'s\", \" 's\" )\n",
    "        s = s.replace(\"n't\", \" not\" )\n",
    "        s = s.replace(\"'re\", \" 're\" )\n",
    "        #print(s+'\\n')\n",
    "        #print(i, s)\n",
    "        out.append(s)\n",
    "        \n",
    "    return out\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "address = widgets.Text(\n",
    "    value='my_input_directory',\n",
    "    placeholder='',\n",
    "    description='Input dir:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "display(address)\n",
    "\n",
    "message = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='',\n",
    "    description='',\n",
    "    disabled=True\n",
    "    \n",
    ")  \n",
    "\n",
    "display(message)\n",
    "\n",
    "button_validate_split = widgets.Button(\n",
    "    value=False,\n",
    "    description='Validate the split',\n",
    "    disabled=False,\n",
    "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "reset_input = widgets.Button(\n",
    "    value=False,\n",
    "    description='Reset input files...',\n",
    "    disabled=False,\n",
    "    button_style='warning', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='',\n",
    "    icon='check',\n",
    "\n",
    ")\n",
    "\n",
    "display(reset_input)\n",
    "reset_input.layout.visibility = 'hidden'\n",
    "\n",
    "display(button_validate_split)\n",
    "button_validate_split.layout.visibility = 'hidden'\n",
    "\n",
    "def handle_add(sender):\n",
    "    reset_input.layout.visibility = 'hidden'\n",
    "    to_dir = \"data/bloomberg/test/mainbody\"\n",
    "    ld = lambda x,y: x + '\\n' + y\n",
    "    \n",
    "    for file in os.listdir(address.value):\n",
    "        #print('\\n' + file)\n",
    "        if file.endswith(\".txt\"):\n",
    "            with open(address.value + \"/\" + str(file), \"r+\", encoding = \"UTF-8\") as ff:\n",
    "                ss = split_text(ff.read())\n",
    "                ss = reduce(ld, ss)\n",
    "                with open(to_dir + \"/\" + str(file), \"w+\", encoding = \"UTF-8\") as fl:\n",
    "                    fl.write(ss)\n",
    "                    \n",
    "    message.value = \"Please verify the split into sentences in the editor, correct it if nesessary and validate it.\"\n",
    "    message.disabled = False\n",
    "    for file in os.listdir(address.value):\n",
    "        os.system(\"start notepad++ \"+ to_dir + \"/\" + str(file))\n",
    "\n",
    "    button_validate_split.layout.visibility = 'visible'\n",
    "    \n",
    "    \n",
    "    \n",
    "def on_c(sender):#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    reset_input.layout.visibility = 'hidden'\n",
    "    message.value = \"Please wait until the end of summarization. This may take several minutes...\"\n",
    "    print(\"Loading the embedding...\")\n",
    "    vocab_dict, word_embedding_array = DataProcessor().prepare_vocab_embeddingdict()\n",
    "    \n",
    "    def find_sentences(s):\n",
    "        #print(s)\n",
    "        previous = 0\n",
    "        L = []\n",
    "        for i, word in enumerate(s):\n",
    "            if (word == '.'):\n",
    "                L.append(s[previous + 1:i + 1])\n",
    "                previous = i\n",
    "        return L\n",
    "    \n",
    "    def print_sentences(L, f):\n",
    "        s = f + \"\\n\"\n",
    "        #print(f)\n",
    "        for sentence in L:\n",
    "\n",
    "            for x in sentence.split(): \n",
    "                try:\n",
    "                    s += str(vocab_dict[x]) + \" \"\n",
    "                except KeyError:\n",
    "                    s += \"1 \"\n",
    "            s += '\\n'\n",
    "        s += \"\\n\"\n",
    "        return s\n",
    "    \n",
    "    \n",
    "    def preprocess(dirr_from, dirr_to):    \n",
    "        s = \"\"\n",
    "        import os\n",
    "        for file in os.listdir(dirr_from):\n",
    "            if file.endswith(\".txt\"):\n",
    "                with open(dirr_from + \"/\" + str(file), \"r\", encoding = \"UTF-8\") as ff:\n",
    "                    q = print_sentences(ff.read().split('\\n'),\"blmbrg-\" + file[:-4])\n",
    "\n",
    "                    s += q\n",
    "\n",
    "        with open(dirr_to + \"/\" + \"bloomberg.test.doc\",\"w+\") as f:\n",
    "            f.write(s)\n",
    "        \n",
    "        \n",
    "    fr = \"data/bloomberg/test/mainbody\"\n",
    "    to = \"data/mypreprocdir\"\n",
    "    preprocess(fr, to)\n",
    "    \n",
    "    print(\"Starting summarization...\")\n",
    "    #os.system(\"python document_summarizer_training_testing.py\")\n",
    "    #subprocess.call([\"python document_summarizer_training_testing.py\"])\n",
    "    filename = \"document_summarizer_training_testing.py\"\n",
    "    %run $filename > file\n",
    "    #%load ./document_summarizer_training_testing.py\n",
    "    \n",
    "    dr =  \"trainingdir/model.ckpt.epoch-\"+ str(my_flags.FLAGS.model_to_load) + \".test.predictions\"\n",
    "    \n",
    "    print(\"Writing summaries...\")\n",
    "    \n",
    "    def print_summaries(dr):\n",
    "        s = {}\n",
    "        q = []\n",
    "\n",
    "        sums = []\n",
    "        with open(dr, \"r\", encoding = 'UTF-8') as f:   \n",
    "            sums = f.read().split(\"\\n\\n\")\n",
    "\n",
    "        summs = {x[0][7:]:[l.split('\\t')[1] for l in x[1:]] for x in [z.split('\\n') for z in sums]}\n",
    "\n",
    "        lexp = lambda x,y: x + '\\n' + y\n",
    "        ''''''\n",
    "\n",
    "        for file in os.listdir(\"data/bloomberg/test/mainbody\"):\n",
    "            if file.endswith(\".txt\"):\n",
    "                with open(\"data/bloomberg/test/mainbody/\" + str(file), \"r+\", encoding = \"UTF-8\") as ff:\n",
    "                    q = ff.read().split('\\n')\n",
    "                #print(len(q), len(summs[file[:-4]]))\n",
    "                assert(len(q) == len(summs[file[:-4]]))\n",
    "                s[file[:-4]] = reduce(lexp, [str(x) + ' ' + y for x,y, in zip(summs[file[:-4]], q)] )\n",
    "\n",
    "        return s\n",
    "    \n",
    "    \n",
    "    s = ''\n",
    "    for file, text in print_summaries(dr).items():\n",
    "        s += file + '\\n' + text + '\\n\\n'\n",
    "    \n",
    "    \n",
    "    with open(\"summaries.txt\", \"w+\", encoding = 'UTF-8') as f:\n",
    "        f.write(s)\n",
    "    \n",
    "    tf.reset_default_graph() \n",
    "\n",
    "\n",
    "    for file in os.listdir(\"data/bloomberg/test/mainbody\"):\n",
    "        os.remove(\"data/bloomberg/test/mainbody/\" + file)\n",
    "    #print_summaries(dr)\n",
    "    message.value = \"Check the results in <summary.txt> in working directory. You can now restart the procedure.\\\n",
    "        If you want to add new articles please update if necessary input directory and press 'Reset input'... \"\n",
    "    reset_input.layout.visibility = 'visible'\n",
    "    \n",
    "    \n",
    "addr = address.on_submit(handle_add)\n",
    "\n",
    "reset_input.on_click(handle_add)\n",
    "\n",
    "button_validate_split.on_click(on_c)\n",
    "\n",
    "message.value = \"Enter the path of your input directory and press <Enter>...\"\n",
    "##################################################\n",
    "\n",
    "#split_text(open('try.txt', 'r').read(), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:condathree]",
   "language": "python",
   "name": "conda-env-condathree-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
