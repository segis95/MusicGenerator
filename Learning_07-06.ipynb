{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout,Flatten, BatchNormalization\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "# load ascii text and covert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "data = pickle.load( open( \"jazz123.txt\", \"rb\" ))\n",
    "lengths = [len(x) for x in data]\n",
    "idx = np.cumsum(lengths)\n",
    "d = sum(data,[])\n",
    "e = to_categorical(d)\n",
    "data_rec = [e[:idx[0]]] + [e[idx[t]:idx[t+1]] for t in range(len(idx) - 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rec[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version #1: 49 -> 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "seq_length = 50\n",
    "\n",
    "for seq in data_rec: \n",
    "    n_chars = len(seq)\n",
    "    for i in range(0, n_chars - seq_length, 1):\n",
    "        if (seq[i][0] == 1.0 or seq[i][1] == 1.0 ):\n",
    "            continue\n",
    "        seq_in = seq[i:i + seq_length]\n",
    "        seq_out = seq[i + seq_length]\n",
    "        dataX.append(seq_in)\n",
    "        dataY.append(seq_out)\n",
    "n_patterns = len(dataX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1362200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version #2: 50 целиком"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1362200"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 100000\n",
    "\n",
    "X = numpy.reshape(dataX[:number], (number, seq_length, 128))\n",
    "y = np.array(dataY[:number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-3e8f45b3202b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data_X_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_pickle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfix_imports\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data_y_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\condathree\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[0;32m    509\u001b[0m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m         format.write_array(fid, arr, allow_pickle=allow_pickle,\n\u001b[1;32m--> 511\u001b[1;33m                            pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[0;32m    512\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mown_fid\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\condathree\\lib\\site-packages\\numpy\\lib\\format.py\u001b[0m in \u001b[0;36mwrite_array\u001b[1;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m             \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    598\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m             for chunk in numpy.nditer(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "number = 10000\n",
    "for i in range(120):\n",
    "    \n",
    "    X = numpy.reshape(dataX[i * number:(i+1) * number], (number, seq_length, 128))\n",
    "    y = np.array(dataY[i * number:(i+1) * number])\n",
    "    \n",
    "    with open(\"data_X_\" + str(i), \"wb\") as fp:  \n",
    "        np.save(fp, X, allow_pickle = True, fix_imports=False)\n",
    "    \n",
    "    with open(\"data_y_\" + str(i), \"wb\") as fp:  \n",
    "        np.save(fp, y, allow_pickle = True, fix_imports=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weights-improvement-{epoch:02d}-{loss:.4f}.hdf5'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(250, input_shape=(X.shape[1], X.shape[2]), return_sequences = True, return_state = False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(225, return_sequences = True, return_state = False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(200, return_sequences = True, return_state = False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(175, return_sequences = True, return_state = False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(150, return_sequences = True, return_state = False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(128, return_sequences = False, return_state = False))#added\n",
    "\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "#model.add(Flatten())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "filepath=\"2Xmodel-{epoch:02d}-{loss:.3f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only = True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "opt = keras.optimizers.Adam()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "128/128 [==============================] - 4s 30ms/step - loss: 4.8941\n",
      "\n",
      "Epoch 00001: loss improved from inf to 4.89408, saving model to 2Xmodel-01-4.894.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2463d46b5c0>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number = len(dataX[0][0])\n",
    "X = numpy.reshape(dataX[:number], (number, seq_length, 128))\n",
    "y = np.array(dataY[:number])\n",
    "model.fit(X, y, epochs = 1, verbose=1, batch_size = 128, callbacks = callbacks_list, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"model-01-1.302.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-110b4b4d2beb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "[1,2,3][0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading data....\n",
      "epoch: 0\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 239s 2ms/step - loss: 0.8717\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.85919\n",
      "downloading data....\n",
      "epoch: 1\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 239s 2ms/step - loss: 0.8557\n",
      "\n",
      "Epoch 00001: loss improved from 0.85919 to 0.85567, saving model to 2Xmodel-01-0.856.hdf5\n",
      "downloading data....\n",
      "epoch: 2\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 240s 2ms/step - loss: 0.8582\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.85567\n",
      "downloading data....\n",
      "epoch: 3\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 241s 2ms/step - loss: 0.8572\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.85567\n",
      "downloading data....\n",
      "epoch: 4\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 240s 2ms/step - loss: 0.8559\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.85567\n",
      "downloading data....\n",
      "epoch: 5\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 239s 2ms/step - loss: 0.8588\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.85567\n",
      "downloading data....\n",
      "epoch: 6\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 238s 2ms/step - loss: 0.8628\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.85567\n",
      "downloading data....\n",
      "epoch: 7\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 239s 2ms/step - loss: 0.8563\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.85567\n",
      "downloading data....\n",
      "epoch: 8\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 239s 2ms/step - loss: 0.8611\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.85567\n",
      "downloading data....\n",
      "epoch: 9\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 241s 2ms/step - loss: 0.8580\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.85567\n",
      "downloading data....\n",
      "epoch: 10\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 240s 2ms/step - loss: 0.8596\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.85567\n",
      "downloading data....\n",
      "epoch: 11\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 239s 2ms/step - loss: 0.8550\n",
      "\n",
      "Epoch 00001: loss improved from 0.85567 to 0.85500, saving model to 2Xmodel-01-0.855.hdf5\n",
      "downloading data....\n",
      "epoch: 12\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 238s 2ms/step - loss: 0.8581\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.85500\n",
      "downloading data....\n",
      "epoch: 13\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 239s 2ms/step - loss: 0.8576\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.85500\n",
      "downloading data....\n",
      "epoch: 14\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 238s 2ms/step - loss: 0.8568\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.85500\n",
      "downloading data....\n",
      "epoch: 15\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 239s 2ms/step - loss: 0.8489\n",
      "\n",
      "Epoch 00001: loss improved from 0.85500 to 0.84891, saving model to 2Xmodel-01-0.849.hdf5\n",
      "downloading data....\n",
      "epoch: 16\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 239s 2ms/step - loss: 0.8483\n",
      "\n",
      "Epoch 00001: loss improved from 0.84891 to 0.84830, saving model to 2Xmodel-01-0.848.hdf5\n",
      "downloading data....\n",
      "epoch: 17\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 238s 2ms/step - loss: 0.8536\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.84830\n",
      "downloading data....\n",
      "epoch: 18\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 240s 2ms/step - loss: 0.8580\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.84830\n",
      "downloading data....\n",
      "epoch: 19\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 238s 2ms/step - loss: 0.8407\n",
      "\n",
      "Epoch 00001: loss improved from 0.84830 to 0.84075, saving model to 2Xmodel-01-0.841.hdf5\n",
      "downloading data....\n",
      "epoch: 20\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 239s 2ms/step - loss: 0.8519\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.84075\n",
      "downloading data....\n",
      "epoch: 21\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 239s 2ms/step - loss: 0.8519\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.84075\n",
      "downloading data....\n",
      "epoch: 22\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 239s 2ms/step - loss: 0.8503\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.84075\n",
      "downloading data....\n",
      "epoch: 23\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 238s 2ms/step - loss: 0.8484\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.84075\n",
      "downloading data....\n",
      "epoch: 24\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 239s 2ms/step - loss: 0.8520\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.84075\n",
      "downloading data....\n",
      "epoch: 25\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 240s 2ms/step - loss: 0.8490\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.84075\n",
      "downloading data....\n",
      "epoch: 26\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 246s 2ms/step - loss: 0.8475\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.84075\n",
      "downloading data....\n",
      "epoch: 27\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s 2ms/step - loss: 0.8400\n",
      "\n",
      "Epoch 00001: loss improved from 0.84075 to 0.84001, saving model to 2Xmodel-01-0.840.hdf5\n",
      "downloading data....\n",
      "epoch: 28\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 240s 2ms/step - loss: 0.8441\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.84001\n",
      "downloading data....\n",
      "epoch: 29\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 238s 2ms/step - loss: 0.8476\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.84001\n",
      "downloading data....\n",
      "epoch: 30\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 241s 2ms/step - loss: 0.8489\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.84001\n",
      "downloading data....\n",
      "epoch: 31\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 239s 2ms/step - loss: 0.8504\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.84001\n",
      "downloading data....\n",
      "epoch: 32\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 239s 2ms/step - loss: 0.8447\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.84001\n",
      "downloading data....\n",
      "epoch: 33\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 239s 2ms/step - loss: 0.8435\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.84001\n",
      "downloading data....\n",
      "epoch: 34\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 240s 2ms/step - loss: 0.8343\n",
      "\n",
      "Epoch 00001: loss improved from 0.84001 to 0.83433, saving model to 2Xmodel-01-0.834.hdf5\n",
      "downloading data....\n",
      "epoch: 35\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 239s 2ms/step - loss: 0.8405\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.83433\n",
      "downloading data....\n",
      "epoch: 36\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 238s 2ms/step - loss: 0.8459\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.83433\n",
      "downloading data....\n",
      "epoch: 37\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 239s 2ms/step - loss: 0.8374\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.83433\n",
      "downloading data....\n",
      "epoch: 38\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 239s 2ms/step - loss: 0.8340\n",
      "\n",
      "Epoch 00001: loss improved from 0.83433 to 0.83402, saving model to 2Xmodel-01-0.834.hdf5\n",
      "downloading data....\n",
      "epoch: 39\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 239s 2ms/step - loss: 0.8405\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.83402\n",
      "downloading data....\n",
      "epoch: 40\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 240s 2ms/step - loss: 0.8344\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.83402\n",
      "downloading data....\n",
      "epoch: 41\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 239s 2ms/step - loss: 0.8383\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.83402\n",
      "downloading data....\n",
      "epoch: 42\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 238s 2ms/step - loss: 0.8337\n",
      "\n",
      "Epoch 00001: loss improved from 0.83402 to 0.83370, saving model to 2Xmodel-01-0.834.hdf5\n",
      "downloading data....\n",
      "epoch: 43\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 238s 2ms/step - loss: 0.8369\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.83370\n",
      "downloading data....\n",
      "epoch: 44\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 239s 2ms/step - loss: 0.8298\n",
      "\n",
      "Epoch 00001: loss improved from 0.83370 to 0.82983, saving model to 2Xmodel-01-0.830.hdf5\n",
      "downloading data....\n",
      "epoch: 45\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 239s 2ms/step - loss: 0.8459\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.82983\n",
      "downloading data....\n",
      "epoch: 46\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000/100000 [==============================] - 235s 2ms/step - loss: 0.8313\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.82983\n",
      "downloading data....\n",
      "epoch: 47\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 236s 2ms/step - loss: 0.8493\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.82983\n",
      "downloading data....\n",
      "epoch: 48\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 240s 2ms/step - loss: 0.8383\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.82983\n",
      "downloading data....\n",
      "epoch: 49\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 239s 2ms/step - loss: 0.8343\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.82983\n",
      "downloading data....\n",
      "epoch: 50\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 238s 2ms/step - loss: 0.8368\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.82983\n",
      "downloading data....\n",
      "epoch: 51\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 238s 2ms/step - loss: 0.8361\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.82983\n",
      "downloading data....\n",
      "epoch: 52\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 238s 2ms/step - loss: 0.8347\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.82983\n",
      "downloading data....\n",
      "epoch: 53\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 245s 2ms/step - loss: 0.8256\n",
      "\n",
      "Epoch 00001: loss improved from 0.82983 to 0.82561, saving model to 2Xmodel-01-0.826.hdf5\n",
      "downloading data....\n",
      "epoch: 54\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 252s 3ms/step - loss: 0.8326\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.82561\n",
      "downloading data....\n",
      "epoch: 55\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 241s 2ms/step - loss: 0.8381\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.82561\n",
      "downloading data....\n",
      "epoch: 56\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 239s 2ms/step - loss: 0.8347\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.82561\n",
      "downloading data....\n",
      "epoch: 57\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 239s 2ms/step - loss: 0.8325\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.82561\n",
      "downloading data....\n",
      "epoch: 58\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 239s 2ms/step - loss: 0.8298\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.82561\n",
      "downloading data....\n",
      "epoch: 59\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 240s 2ms/step - loss: 0.8249\n",
      "\n",
      "Epoch 00001: loss improved from 0.82561 to 0.82490, saving model to 2Xmodel-01-0.825.hdf5\n",
      "downloading data....\n",
      "epoch: 60\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 238s 2ms/step - loss: 0.8289\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.82490\n",
      "downloading data....\n",
      "epoch: 61\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 238s 2ms/step - loss: 0.8381\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.82490\n",
      "downloading data....\n",
      "epoch: 62\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 237s 2ms/step - loss: 0.8345\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.82490\n",
      "downloading data....\n",
      "epoch: 63\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 238s 2ms/step - loss: 0.8255\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.82490\n",
      "downloading data....\n",
      "epoch: 64\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 236s 2ms/step - loss: 0.8278\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.82490\n",
      "downloading data....\n",
      "epoch: 65\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 237s 2ms/step - loss: 0.8301\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.82490\n",
      "downloading data....\n",
      "epoch: 66\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 237s 2ms/step - loss: 0.8253\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.82490\n",
      "downloading data....\n",
      "epoch: 67\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 237s 2ms/step - loss: 0.8339\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.82490\n",
      "downloading data....\n",
      "epoch: 68\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 237s 2ms/step - loss: 0.8131\n",
      "\n",
      "Epoch 00001: loss improved from 0.82490 to 0.81312, saving model to 2Xmodel-01-0.813.hdf5\n",
      "downloading data....\n",
      "epoch: 69\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 238s 2ms/step - loss: 0.8260\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.81312\n",
      "downloading data....\n",
      "epoch: 70\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 237s 2ms/step - loss: 0.8257\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.81312\n",
      "downloading data....\n",
      "epoch: 71\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 236s 2ms/step - loss: 0.8252\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.81312\n",
      "downloading data....\n",
      "epoch: 72\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 236s 2ms/step - loss: 0.8276\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.81312\n",
      "downloading data....\n",
      "epoch: 73\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 236s 2ms/step - loss: 0.8274\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.81312\n",
      "downloading data....\n",
      "epoch: 74\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 236s 2ms/step - loss: 0.8232\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.81312\n",
      "downloading data....\n",
      "epoch: 75\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 237s 2ms/step - loss: 0.8287\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.81312\n",
      "downloading data....\n",
      "epoch: 76\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 236s 2ms/step - loss: 0.8282\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.81312\n",
      "downloading data....\n",
      "epoch: 77\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 237s 2ms/step - loss: 0.8261\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.81312\n",
      "downloading data....\n",
      "epoch: 78\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 237s 2ms/step - loss: 0.8255\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.81312\n",
      "downloading data....\n",
      "epoch: 79\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 236s 2ms/step - loss: 0.8244\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.81312\n",
      "downloading data....\n",
      "epoch: 80\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 236s 2ms/step - loss: 0.8170\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.81312\n",
      "downloading data....\n",
      "epoch: 81\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 236s 2ms/step - loss: 0.8137\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.81312\n",
      "downloading data....\n",
      "epoch: 82\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 237s 2ms/step - loss: 0.8207\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.81312\n",
      "downloading data....\n",
      "epoch: 83\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 236s 2ms/step - loss: 0.8245\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.81312\n",
      "downloading data....\n",
      "epoch: 84\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 237s 2ms/step - loss: 0.8233\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.81312\n",
      "downloading data....\n",
      "epoch: 85\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 237s 2ms/step - loss: 0.8210\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.81312\n",
      "downloading data....\n",
      "epoch: 86\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s 2ms/step - loss: 0.8166\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.81312\n",
      "downloading data....\n",
      "epoch: 87\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 239s 2ms/step - loss: 0.8168\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.81312\n",
      "downloading data....\n",
      "epoch: 88\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 239s 2ms/step - loss: 0.8221\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.81312\n",
      "downloading data....\n",
      "epoch: 89\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 238s 2ms/step - loss: 0.8102\n",
      "\n",
      "Epoch 00001: loss improved from 0.81312 to 0.81023, saving model to 2Xmodel-01-0.810.hdf5\n",
      "downloading data....\n",
      "epoch: 90\n",
      "Epoch 1/1\n",
      "  5120/100000 [>.............................] - ETA: 3:56 - loss: 0.8105"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-162-418547cb3b39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"epoch: %d\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[1;31m#np.random.shuffle(L)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\condathree\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1002\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\condathree\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\condathree\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1236\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1237\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\condathree\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2482\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2483\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\condathree\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\condathree\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\condathree\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\condathree\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\condathree\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\condathree\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "number = 100000\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    print(\"downloading data....\")\n",
    "    rando = np.random.choice(range(len(dataX)), number, replace = False)\n",
    "    \n",
    "    X = numpy.reshape([dataX[i] for i in rando], (number, seq_length, 128))\n",
    "    y = np.array([dataY[i] for i in rando])\n",
    "    \n",
    "    print(\"epoch: %d\"%(epoch))\n",
    "    \n",
    "    model.fit(X, y, epochs = 1, verbose=1, batch_size = 128, callbacks = callbacks_list, shuffle = True)\n",
    "    #np.random.shuffle(L)\n",
    "    \n",
    "    \n",
    "    #for j, i in enumerate(L):\n",
    "        \n",
    "        \n",
    "    #    print(\"downloading data....\")\n",
    "        \n",
    "    #    X = numpy.reshape(dataX[i * number:(i+1) * number], (number, seq_length, 128))\n",
    "    #    y = np.array(dataY[i * number:(i+1) * number])\n",
    "        \n",
    "        \n",
    "        #print(\"epoch: %d, part: %d, number: %d\" % (epoch, j, i))\n",
    "        #model.fit(X, y, epochs = 1, verbose=1, batch_size = 128, callbacks = callbacks_list, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46667632], dtype=float32)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(prediction, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100,  42,   7,   5,   2,   2,   1,   1,   1,   1,   1,   1,   1,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(prediction[0][sorted(range(128), key = lambda x:prediction[0,x], reverse = True)] / np.max(prediction, axis = 1) * 100, \"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "rando = np.random.choice(range(1300000), 100000, replace=False)\n",
    "print(len(set(rando)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"total_sunday.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 + 2'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"%d + %d\"%(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 261s 3ms/step - loss: 1.7632\n",
      "Epoch 2/2\n",
      " 71168/100000 [====================>.........] - ETA: 1:15 - loss: 1.6806"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-013b156d555a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\condathree\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 960\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\condathree\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1648\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1649\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1650\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\condathree\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\condathree\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2352\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2353\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\condathree\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\condathree\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\condathree\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\condathree\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\condathree\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\condathree\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs = 2, verbose=1, batch_size = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_in = X[0].reshape((1, 50, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.02089757e-04, 7.29699731e-01, 4.81484392e-07, 5.07727975e-07,\n",
       "        4.65268670e-07, 4.97117639e-07, 5.17919318e-07, 4.38293711e-07,\n",
       "        4.94290703e-07, 5.40053734e-05, 4.33493227e-07, 4.24668684e-07,\n",
       "        4.55670772e-07, 5.74965270e-07, 4.84270458e-07, 4.57068722e-07,\n",
       "        5.48520063e-07, 4.43684343e-07, 4.98198290e-07, 4.62957814e-07,\n",
       "        4.50848489e-07, 1.07666267e-06, 5.57208296e-06, 2.07643416e-06,\n",
       "        6.50653919e-06, 6.11844598e-06, 1.63248351e-05, 2.08976446e-04,\n",
       "        6.75121919e-05, 1.13725034e-03, 2.89812335e-04, 2.44551688e-03,\n",
       "        9.87526611e-04, 7.16817472e-03, 5.92686329e-03, 5.87560795e-03,\n",
       "        5.66303264e-03, 3.21254390e-03, 3.52350287e-02, 5.68942493e-03,\n",
       "        7.39225559e-03, 3.86381447e-02, 1.42651238e-03, 7.06781447e-02,\n",
       "        6.86901622e-03, 3.45782414e-02, 1.13773420e-02, 2.76215211e-03,\n",
       "        1.95123709e-03, 2.37547420e-03, 6.32626330e-03, 6.03665481e-04,\n",
       "        1.73663010e-03, 8.64834583e-04, 9.80679106e-05, 7.72825093e-04,\n",
       "        3.04366898e-04, 5.99427149e-04, 1.70733940e-04, 1.31494322e-04,\n",
       "        1.96690933e-04, 8.94972254e-05, 1.21792036e-04, 1.32973815e-04,\n",
       "        1.87638871e-04, 1.87880083e-04, 1.23531252e-04, 4.64966433e-04,\n",
       "        5.04056989e-05, 2.20554575e-04, 3.35613819e-04, 8.47847914e-05,\n",
       "        3.93095543e-04, 3.58842553e-05, 1.17636395e-04, 3.27717571e-04,\n",
       "        4.78901842e-04, 5.31993399e-04, 4.02558187e-04, 2.82978435e-04,\n",
       "        2.96721526e-04, 1.83110169e-04, 2.11808918e-04, 1.13216047e-04,\n",
       "        1.19589778e-04, 1.65409379e-04, 8.32999867e-05, 5.75185259e-05,\n",
       "        1.74001852e-05, 2.15571708e-05, 4.55020745e-05, 6.86798085e-05,\n",
       "        1.48255667e-05, 2.96529579e-05, 2.71340668e-05, 3.54040958e-06,\n",
       "        5.07519362e-05, 1.19578199e-05, 6.76002946e-06, 2.58301361e-06,\n",
       "        5.52227084e-06, 3.03573006e-06, 3.11542385e-06, 9.18185833e-07,\n",
       "        3.63824097e-06, 1.77157028e-06, 6.11440373e-06, 4.78523589e-07,\n",
       "        1.68248926e-06, 4.67346325e-07, 4.48551901e-07, 4.96146242e-07,\n",
       "        4.86859051e-07, 5.30390025e-07, 4.35331003e-07, 4.43791436e-07,\n",
       "        4.08969299e-07, 5.49709910e-07, 4.82815096e-07, 5.31231876e-07,\n",
       "        5.33155969e-07, 4.52595373e-07, 5.45774412e-07, 5.02575233e-07,\n",
       "        4.82490577e-07, 4.25172772e-07, 5.52959818e-07, 4.43481724e-07]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice([1,2,3,4,5,6,7,8,9,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_one(pred):\n",
    "    a = np.log(pred[0]) / 1.0 \n",
    "    dist = np.exp(a)/np.sum(np.exp(a)) \n",
    "    choices = range(len(a))\n",
    "    return np.random.choice(choices, p=dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100  57  11   9   7   7   3   1   1   1]\n",
      "[100  19   7   3   3   2   1   1   0   0]\n",
      "[100  92  40  38  11   6   3   3   2   1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Charles\\Anaconda3\\envs\\condathree\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100  17   7   7   2   1   1   0   0   0]\n",
      "[100  43  22  20  20   5   5   3   3   1]\n",
      "[100  17  14  12  10   5   3   1   0   0]\n",
      "[100  11   8   4   3   1   0   0   0   0]\n",
      "[100  58  38  20  14  11   5   1   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100  71  46  45  17   5   4   3   2   1]\n",
      "[100   1   1   0   0   0   0   0   0   0]\n",
      "[100  36  20  17  13   5   4   3   2   1]\n",
      "[100  53  25  25  10   5   4   2   2   2]\n",
      "[100  24  15   5   4   3   2   2   2   1]\n",
      "[100  27   6   4   3   2   2   1   1   1]\n",
      "[100  26  12   7   5   3   3   2   2   2]\n",
      "[100  10   5   5   3   2   2   2   1   1]\n",
      "[100   4   4   2   2   1   1   1   0   0]\n",
      "[100   4   3   2   1   1   1   1   0   0]\n",
      "[100   8   4   3   2   2   2   1   1   1]\n",
      "[100  26  13   7   4   4   4   3   3   2]\n",
      "[100  81  44  29  18  18  14  10   8   7]\n",
      "[100   9   3   1   0   0   0   0   0   0]\n",
      "[100  58   5   3   3   2   2   2   1   1]\n",
      "[100  96   5   4   3   3   2   2   2   2]\n",
      "[100  14   0   0   0   0   0   0   0   0]\n",
      "[100  25   2   1   0   0   0   0   0   0]\n",
      "[100  40   5   3   1   1   1   1   0   0]\n",
      "[100  63   8   7   3   2   1   1   0   0]\n",
      "[100  39   9   4   3   2   1   1   0   0]\n",
      "[100   8   2   1   1   1   0   0   0   0]\n",
      "[100  14   3   1   1   0   0   0   0   0]\n",
      "[100  23   3   2   2   2   1   1   0   0]\n",
      "[100  73  25  14   8   7   7   4   3   3]\n",
      "[100   9   4   2   1   0   0   0   0   0]\n",
      "[100   8   2   1   1   1   0   0   0   0]\n",
      "[100  53  13  13  11   6   5   4   3   2]\n",
      "[100  99  16  13  13  11   9   5   3   2]\n",
      "[100   6   4   3   3   1   1   1   0   0]\n",
      "[100  48  27  24  13  10   5   4   2   1]\n",
      "[100   6   5   3   2   1   1   1   0   0]\n",
      "[100  93  41  28  16   7   7   6   6   6]\n",
      "[100  52  27  14   9   6   4   3   3   2]\n",
      "[100   4   3   1   1   1   0   0   0   0]\n",
      "[100  69  45  32  29  14   6   4   3   1]\n",
      "[100   2   1   0   0   0   0   0   0   0]\n",
      "[100  26   7   4   3   2   1   1   1   1]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100  32  23  14  11   9   5   4   3   1]\n",
      "[100  24  17  17  13   5   4   2   1   1]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100  39  33  24  21  19  16   9   8   8]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  54   6   6   6   5   4   3   2   2]\n",
      "[100  38  10   9   5   4   3   3   2   1]\n",
      "[100   9   1   0   0   0   0   0   0   0]\n",
      "[100   6   1   1   0   0   0   0   0   0]\n",
      "[100  92  22  19  13   9   6   5   5   2]\n",
      "[100   2   1   1   0   0   0   0   0   0]\n",
      "[100   3   2   2   1   1   1   0   0   0]\n",
      "[100   7   3   2   1   1   1   0   0   0]\n",
      "[100   6   3   1   1   1   0   0   0   0]\n",
      "[100  16  15  10   9   6   3   3   1   1]\n",
      "[100  31  29  22  18  16  11   4   4   3]\n",
      "[100   1   1   0   0   0   0   0   0   0]\n",
      "[100  33   4   1   1   0   0   0   0   0]\n",
      "[100  18  11  10   8   4   3   3   2   1]\n",
      "[100   8   7   7   5   4   2   2   2   2]\n",
      "[100  10   3   2   1   1   1   1   0   0]\n",
      "[100  73  24  17  15  13   5   5   5   3]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100  68  18  13   7   5   3   2   1   1]\n",
      "[100  73  33  11   5   3   3   3   1   1]\n",
      "[100  94  22   8   5   4   4   2   1   1]\n",
      "[100  33  17   5   4   3   3   3   1   1]\n",
      "[100  11   8   3   2   2   1   1   1   1]\n",
      "[100  17   7   2   2   1   1   1   1   1]\n",
      "[100  14   3   3   2   1   1   0   0   0]\n",
      "[100   4   3   2   1   1   1   0   0   0]\n",
      "[100  25  21  18  14   6   2   2   2   2]\n",
      "[100  16   9   4   4   2   2   1   0   0]\n",
      "[100   5   4   3   2   1   1   0   0   0]\n",
      "[100  12   5   5   4   3   3   0   0   0]\n",
      "[100  29  17  11  10   8   6   3   2   2]\n",
      "[100  16  12   8   6   2   1   1   1   1]\n",
      "[100   3   2   2   1   1   0   0   0   0]\n",
      "[100  22  17  12   5   3   3   3   1   1]\n",
      "[100  26  19   7   4   4   4   4   2   1]\n",
      "[100   9   4   1   0   0   0   0   0   0]\n",
      "[100  85  22  12   9   2   2   1   1   1]\n",
      "[100  24   3   2   1   1   1   0   0   0]\n",
      "[100  12   4   2   1   1   0   0   0   0]\n",
      "[100  15   5   1   1   1   0   0   0   0]\n",
      "[100  17   1   0   0   0   0   0   0   0]\n",
      "[100  21   1   1   0   0   0   0   0   0]\n",
      "[100  55   3   2   1   1   0   0   0   0]\n",
      "[100  37   1   1   1   1   0   0   0   0]\n",
      "[100  21   1   1   0   0   0   0   0   0]\n",
      "[100  24   6   2   1   1   1   1   0   0]\n",
      "[100  87  51  19  18  16  14   8   7   5]\n",
      "[100   4   1   0   0   0   0   0   0   0]\n",
      "[100   2   2   1   1   0   0   0   0   0]\n",
      "[100  46  34  33  27  17  12   9   2   2]\n",
      "[100  49  33  17  14   7   5   2   2   1]\n",
      "[100  11   5   4   4   4   2   0   0   0]\n",
      "[100  20   7   6   4   3   3   1   1   0]\n",
      "[100  11   4   4   2   2   1   1   1   1]\n",
      "[100  13  11   6   5   2   1   0   0   0]\n",
      "[100  26  16   9   5   1   1   0   0   0]\n",
      "[100  31  22   7   7   4   4   2   2   1]\n",
      "[100  11   9   4   2   1   0   0   0   0]\n",
      "[100  10   4   3   1   1   1   0   0   0]\n",
      "[100   4   3   3   0   0   0   0   0   0]\n",
      "[100   6   3   2   0   0   0   0   0   0]\n",
      "[100   4   2   1   0   0   0   0   0   0]\n",
      "[100   4   1   1   0   0   0   0   0   0]\n",
      "[100  55  12   7   3   2   2   2   1   1]\n",
      "[100  59  42  14  11  11  10   9   9   8]\n",
      "[100   4   3   1   1   0   0   0   0   0]\n",
      "[100  10   4   2   2   0   0   0   0   0]\n",
      "[100   7   4   3   3   2   1   1   0   0]\n",
      "[100  11   6   2   2   1   1   1   0   0]\n",
      "[100  22   6   4   4   4   2   1   1   0]\n",
      "[100   2   2   2   1   0   0   0   0   0]\n",
      "[100   5   3   1   1   0   0   0   0   0]\n",
      "[100   4   2   1   1   0   0   0   0   0]\n",
      "[100  11   5   2   1   1   0   0   0   0]\n",
      "[100   6   3   3   1   0   0   0   0   0]\n",
      "[100   5   2   1   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100  15   5   2   2   2   1   1   1   1]\n",
      "[100  10   4   4   2   2   2   1   1   1]\n",
      "[100   9   2   1   1   1   0   0   0   0]\n",
      "[100  15  10   7   7   6   3   3   2   2]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  20   0   0   0   0   0   0   0   0]\n",
      "[100  30   0   0   0   0   0   0   0   0]\n",
      "[100  36   7   5   2   1   1   1   1   1]\n",
      "[100   1   1   1   0   0   0   0   0   0]\n",
      "[100   9   3   1   1   1   1   0   0   0]\n",
      "[100   5   1   1   0   0   0   0   0   0]\n",
      "[100   6   2   2   2   2   2   2   2   1]\n",
      "[100  16   7   3   2   2   1   1   1   1]\n",
      "[100   7   2   2   1   1   1   1   1   1]\n",
      "[100  39  12   5   4   4   4   3   3   3]\n",
      "[100  27  10   7   6   4   4   3   2   2]\n",
      "[100  77  13   7   5   4   4   3   3   2]\n",
      "[100   4   2   1   1   1   1   1   1   0]\n",
      "[100   8   6   4   4   3   3   2   2   1]\n",
      "[100   7   1   1   1   1   1   0   0   0]\n",
      "[100   5   5   4   3   2   2   1   1   1]\n",
      "[100  14  13  11   7   6   6   5   3   3]\n",
      "[100   4   2   2   2   1   1   1   1   1]\n",
      "[100   9   4   3   2   1   1   1   1   0]\n",
      "[100  27  19  17  10   8   7   6   6   6]\n",
      "[100  18  12   8   5   4   4   4   3   3]\n",
      "[100   2   1   0   0   0   0   0   0   0]\n",
      "[100  13   8   7   3   3   3   2   1   1]\n",
      "[100  46  21  20  18  18  14  14  11  10]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100  28   8   3   2   1   1   1   1   1]\n",
      "[100  51  32   8   8   8   7   7   6   5]\n",
      "[100  35  10   4   3   3   3   2   2   2]\n",
      "[100  12   6   5   4   4   3   3   2   1]\n",
      "[100  58  34  34  24  13   9   6   6   5]\n",
      "[100   5   2   1   1   1   1   1   1   0]\n",
      "[100  13  10   8   7   6   5   5   4   4]\n",
      "[100  62  22  17  15  14  14   9   8   7]\n",
      "[100   1   1   1   1   1   0   0   0   0]\n",
      "[100  80   5   5   3   2   2   2   1   1]\n",
      "[100  64  44  43  31  24  15  15  13  10]\n",
      "[100  18   5   4   4   3   2   2   1   1]\n",
      "[100  56  26  12   8   8   6   4   3   3]\n",
      "[100   1   1   1   1   1   1   0   0   0]\n",
      "[100   7   4   3   1   1   1   1   0   0]\n",
      "[100  58  30  14  10   9   6   5   5   4]\n",
      "[100  34  24  13   8   8   8   7   6   5]\n",
      "[100   2   1   1   1   1   1   0   0   0]\n",
      "[100  31  11   8   6   5   4   4   4   3]\n",
      "[100   6   3   2   2   1   1   1   1   1]\n",
      "[100  54  25  23  15   6   5   4   4   3]\n",
      "[100  26  12   8   7   6   2   2   2   2]\n",
      "[100  76  69  25  15  12  11   8   8   6]\n",
      "[100   3   1   1   0   0   0   0   0   0]\n",
      "[100  34  34   8   7   7   6   6   5   4]\n",
      "[100  54   8   7   6   6   6   5   4   3]\n",
      "[100  49  12   7   6   5   5   4   4   4]\n",
      "[100   5   3   1   1   1   1   1   0   0]\n",
      "[100  27  16  11   6   4   2   2   1   1]\n",
      "[100   8   4   1   1   1   1   0   0   0]\n",
      "[100  43  36  11  11   7   6   6   5   4]\n",
      "[100   4   3   1   1   1   1   0   0   0]\n",
      "[100  19   7   7   2   1   1   1   1   1]\n",
      "[100  16  10   9   6   3   3   2   2   2]\n",
      "[100  50  35  12   8   6   5   4   4   4]\n",
      "[100   5   2   2   1   1   1   0   0   0]\n",
      "[100  27  13   5   3   2   2   2   1   1]\n",
      "[100  16   3   2   2   1   1   1   1   0]\n",
      "[100  44  13  12  11  10   9   8   6   4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100   6   5   4   3   2   2   1   1   1]\n",
      "[100  30  22  18  13  11   8   6   5   2]\n",
      "[100   3   1   1   1   0   0   0   0   0]\n",
      "[100  41  27  15  10   9   8   7   7   6]\n",
      "[100   6   3   2   2   1   1   1   0   0]\n",
      "[100  21  11   6   4   3   1   1   1   1]\n",
      "[100   5   3   1   1   1   1   0   0   0]\n",
      "[100  12   9   4   3   3   3   2   2   2]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  12   4   3   1   0   0   0   0   0]\n",
      "[100  42  25  19  17   4   2   1   1   1]\n",
      "[100   5   2   2   1   1   0   0   0   0]\n",
      "[100  21  19  15   6   3   2   2   2   2]\n",
      "[100   7   5   4   4   3   3   1   1   1]\n",
      "[100   3   3   2   2   1   1   1   1   1]\n",
      "[100  21   5   4   4   4   4   3   2   2]\n",
      "[100  16   5   4   3   2   2   2   2   2]\n",
      "[100  24  21  17  14  13  12  11   9   9]\n",
      "[100  19  10   8   5   5   5   4   4   4]\n",
      "[100  14  13  12   8   7   5   4   4   3]\n",
      "[100  10   5   5   5   1   0   0   0   0]\n",
      "[100   4   2   2   2   2   1   1   1   1]\n",
      "[100   5   3   1   1   0   0   0   0   0]\n",
      "[100  14   9   8   7   6   4   4   3   3]\n",
      "[100   3   2   1   1   1   0   0   0   0]\n",
      "[100  40  25  21  12   8   6   5   5   4]\n",
      "[100   8   6   3   2   1   1   1   1   0]\n",
      "[100  43  17   7   7   6   5   5   3   2]\n",
      "[100   9   5   1   1   1   1   1   1   1]\n",
      "[100  98  15  13   9   5   4   2   2   2]\n",
      "[100   8   6   4   2   2   2   1   1   1]\n",
      "[100  34   8   5   4   4   4   3   3   2]\n",
      "[100   6   4   4   4   3   3   2   2   2]\n",
      "[100   8   5   4   3   2   2   2   2   1]\n",
      "[100  74  54  26  21  17  15  14  13  11]\n",
      "[100   7   3   3   2   2   2   1   1   1]\n",
      "[100  94  62  40  38  29  27  24  17  16]\n",
      "[100   2   2   2   1   1   1   0   0   0]\n",
      "[100  60  39  30  21  18  14  13  12  11]\n",
      "[100  57  25  15  14  14  12  12  11  10]\n",
      "[100  11   9   6   5   4   4   3   3   3]\n",
      "[100  48  21  18  16  14  13  13  13  11]\n",
      "[100  18   7   5   5   5   4   4   4   4]\n",
      "[100   7   5   4   2   2   2   2   0   0]\n",
      "[100  89  59  58  30  30  26  22  22  18]\n",
      "[100  11   8   4   4   2   2   2   2   2]\n",
      "[100  59  49  49  27  27  27  25  22  17]\n",
      "[100   4   3   2   1   1   1   1   1   0]\n",
      "[100  85  72  25  22  11   9   9   8   8]\n",
      "[100  89  54  21  13  12  10  10  10   8]\n",
      "[100  46  37   9   8   7   6   6   6   4]\n",
      "[100  72  59  19  16  15  14  14  11   9]\n",
      "[100   9   8   4   4   4   3   3   2   2]\n",
      "[100   5   5   5   5   4   4   4   3   3]\n",
      "[100  35  22  18  16  14  14  13  11  11]\n",
      "[100   9   9   7   6   3   3   3   2   2]\n",
      "[100   8   7   6   4   3   3   3   2   2]\n",
      "[100  39  35  25  19  18  15  14   8   7]\n",
      "[100  19  17  12  10   8   6   6   5   4]\n",
      "[100   8   7   5   4   4   3   2   2   2]\n",
      "[100  13  10   4   4   3   3   3   2   2]\n",
      "[100  19   9   5   5   4   3   3   3   3]\n",
      "[100  32  13  13   5   4   4   4   4   3]\n",
      "[100  28  13   5   5   5   4   4   3   3]\n",
      "[100  68  54  28  26  15  14  13  12  11]\n",
      "[100  17  12   6   3   3   2   2   2   2]\n",
      "[100  26  23  11   8   8   5   5   5   5]\n",
      "[100  10   7   6   4   4   4   3   2   1]\n",
      "[100  22  22  14  10   7   6   5   4   3]\n",
      "[100  44  14  14   7   5   5   3   3   3]\n",
      "[100  24   8   7   6   3   3   3   3   2]\n",
      "[100  27  15  11   8   6   6   5   4   3]\n",
      "[100  13  10  10   5   4   4   3   2   2]\n",
      "[100  11   8   3   2   2   1   1   1   1]\n",
      "[100  39  12  11  10   5   5   4   4   2]\n",
      "[100  46  14   6   5   5   4   3   3   3]\n",
      "[100  40   6   6   3   3   3   1   1   1]\n",
      "[100  27   4   4   2   2   2   2   2   1]\n",
      "[100  31   4   3   2   2   2   1   1   1]\n",
      "[100  16   5   1   1   1   1   1   0   0]\n",
      "[100  57  18   9   7   6   5   5   3   3]\n",
      "[100  41   5   5   4   3   3   2   1   1]\n",
      "[100  17   6   2   2   2   1   1   1   0]\n",
      "[100   7   5   4   2   2   1   1   1   0]\n",
      "[100  25  10   6   5   4   4   3   2   2]\n",
      "[100  15  14   7   6   5   4   4   4   4]\n",
      "[100  13  12   8   6   4   3   3   3   2]\n",
      "[100  23  18  14  12  10  10   8   8   7]\n",
      "[100   5   5   5   3   3   3   2   2   2]\n",
      "[100   4   2   2   2   2   1   1   1   1]\n",
      "[100   4   4   3   2   1   1   1   0   0]\n",
      "[100   2   2   2   1   1   1   1   0   0]\n",
      "[100   6   2   1   1   0   0   0   0   0]\n",
      "[100  38  35  18  17  12  10   8   5   4]\n",
      "[100   6   2   1   1   1   1   0   0   0]\n",
      "[100  86  10   6   5   3   3   3   3   1]\n",
      "[100   3   1   1   1   0   0   0   0   0]\n",
      "[100  42  37  19  15  10   6   5   4   4]\n",
      "[100  28   9   9   7   6   2   2   2   1]\n",
      "[100  16  10   9   9   3   3   2   2   1]\n",
      "[100  14  11   5   4   2   2   2   2   1]\n",
      "[100  18   3   2   1   1   1   1   1   1]\n",
      "[100  29   2   2   2   2   1   1   0   0]\n",
      "[100   6   1   1   0   0   0   0   0   0]\n",
      "[100   6   1   0   0   0   0   0   0   0]\n",
      "[100   6   1   1   0   0   0   0   0   0]\n",
      "[100  20   8   7   6   5   1   1   1   1]\n",
      "[100  63  43  25  24  21  19  16  14   7]\n",
      "[100   3   3   3   1   1   0   0   0   0]\n",
      "[100  23  18  14  14  11   3   3   3   2]\n",
      "[100  31  14  10  10  10   7   4   4   2]\n",
      "[100   6   4   2   2   1   1   1   1   0]\n",
      "[100   4   3   3   2   0   0   0   0   0]\n",
      "[100  19   5   4   2   2   1   1   1   1]\n",
      "[100  92  60  13   6   5   5   5   4   4]\n",
      "[100  11   2   2   2   1   1   0   0   0]\n",
      "[100  19  11   6   5   2   2   1   1   1]\n",
      "[100  14   5   2   1   1   1   1   1   0]\n",
      "[100  27  24  17   8   8   7   5   4   4]\n",
      "[100  31  29  25  17  16   8   7   6   6]\n",
      "[100  19  12  10   9   8   7   7   6   6]\n",
      "[100   6   5   4   4   4   3   2   2   1]\n",
      "[100   9   7   5   5   5   3   2   2   1]\n",
      "[100  56  18   8   3   2   2   2   1   1]\n",
      "[100  49  31  17  16  12  12   7   7   6]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100  52   1   0   0   0   0   0   0   0]\n",
      "[100  10   8   3   1   1   1   1   1   0]\n",
      "[100  18  13  10   5   3   2   2   1   1]\n",
      "[100   2   1   1   0   0   0   0   0   0]\n",
      "[100  27  19  11  10   7   3   3   2   2]\n",
      "[100   6   6   3   2   2   2   1   0   0]\n",
      "[100   5   2   2   1   1   0   0   0   0]\n",
      "[100  21  19  19   5   4   3   3   3   2]\n",
      "[100  67  60  60  56  41  32  14  12  11]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  32   0   0   0   0   0   0   0   0]\n",
      "[100  14   1   1   1   1   0   0   0   0]\n",
      "[100  22   3   2   1   1   1   1   0   0]\n",
      "[100  89  80  60  45  33  19  18  14  13]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   7   0   0   0   0   0   0   0   0]\n",
      "[100  35   0   0   0   0   0   0   0   0]\n",
      "[100   7   2   2   1   0   0   0   0   0]\n",
      "[100   6   3   3   3   1   1   1   1   1]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   1   1   1   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   9   1   0   0   0   0   0   0   0]\n",
      "[100  66  28  22  17  15  14   9   4   3]\n",
      "[100  10   6   4   2   2   1   1   1   1]\n",
      "[100   2   2   2   1   0   0   0   0   0]\n",
      "[100  74  20  20   9   6   5   4   3   3]\n",
      "[100  99  75  65  31  19  12  10   9   9]\n",
      "[100   7   6   4   2   2   1   1   1   1]\n",
      "[100  29  19  14  14   6   5   3   3   3]\n",
      "[100  33  29  26  24  12  10  10   9   9]\n",
      "[100   4   3   2   1   1   1   0   0   0]\n",
      "[100  14   7   3   1   1   1   0   0   0]\n",
      "[100  57  15   9   7   7   4   4   3   2]\n",
      "[100  60  16  13  13  12   9   8   3   3]\n",
      "[100  23   9   5   3   2   2   1   1   1]\n",
      "[100  28   4   3   2   2   1   1   1   1]\n",
      "[100  13   4   3   3   2   2   2   1   1]\n",
      "[100  29   0   0   0   0   0   0   0   0]\n",
      "[100  97  45  23  17  15  14  13  11   8]\n",
      "[100  12   5   4   3   3   2   2   2   2]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100  95  11   3   2   1   0   0   0   0]\n",
      "[100  53  30  28  24  18  15  14  12   9]\n",
      "[100   4   3   2   1   1   1   1   1   1]\n",
      "[100  17  14  13   8   7   7   6   6   5]\n",
      "[100   1   1   0   0   0   0   0   0   0]\n",
      "[100  25   7   1   1   1   0   0   0   0]\n",
      "[100  56  38  26  11  11   7   6   5   2]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  97   1   0   0   0   0   0   0   0]\n",
      "[100  59  42   9   5   5   4   2   2   0]\n",
      "[100  27  15   6   3   1   1   1   0   0]\n",
      "[100  63  43  34   6   6   5   5   3   2]\n",
      "[100  74  53  12  10   6   5   4   4   4]\n",
      "[100   6   4   2   1   1   0   0   0   0]\n",
      "[100   5   3   1   0   0   0   0   0   0]\n",
      "[100   7   4   2   1   0   0   0   0   0]\n",
      "[100  10   8   7   2   1   1   1   1   1]\n",
      "[100   5   3   3   3   2   1   1   1   0]\n",
      "[100  88  56  11   6   3   3   1   1   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100  85  60  47  36   6   5   4   3   2]\n",
      "[100  90  28  25  19  16  14  13  10  10]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100  97  10   0   0   0   0   0   0   0]\n",
      "[100  39  20  14   5   4   2   2   1   1]\n",
      "[100  36  30  12   3   2   2   1   1   1]\n",
      "[100  37  21  12   2   1   1   1   0   0]\n",
      "[100  61  39  17  17  15  13  12   2   2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  83   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   4   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100   2   1   0   0   0   0   0   0   0]\n",
      "[100  84  54  46  40  38  34  31  26  13]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  19   3   1   0   0   0   0   0   0]\n",
      "[100  99  33  12  10   8   5   2   1   1]\n",
      "[100  13   4   1   1   1   0   0   0   0]\n",
      "[100  49  14   6   4   3   3   0   0   0]\n",
      "[100  38  14   1   1   1   0   0   0   0]\n",
      "[100  68  35  28   7   4   2   1   1   0]\n",
      "[100  43   5   2   2   2   1   1   0   0]\n",
      "[100  41   4   4   4   3   2   2   1   1]\n",
      "[100  13   2   1   0   0   0   0   0   0]\n",
      "[100  34  31  13   2   2   2   2   2   1]\n",
      "[100  46  14   6   3   1   1   1   0   0]\n",
      "[100  93  39  38  25  23  17  15   7   5]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  28   0   0   0   0   0   0   0   0]\n",
      "[100   7   0   0   0   0   0   0   0   0]\n",
      "[100   8   3   1   0   0   0   0   0   0]\n",
      "[100  10   6   0   0   0   0   0   0   0]\n",
      "[100   4   1   0   0   0   0   0   0   0]\n",
      "[100   3   2   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100   4   2   1   0   0   0   0   0   0]\n",
      "[100  17  16  12  10  10   9   7   4   3]\n",
      "[100  74  15  10   7   6   5   4   3   2]\n",
      "[100  16   4   1   1   1   1   1   0   0]\n",
      "[100   7   4   3   2   1   1   1   0   0]\n",
      "[100   3   1   1   0   0   0   0   0   0]\n",
      "[100   1   1   0   0   0   0   0   0   0]\n",
      "[100   3   1   1   0   0   0   0   0   0]\n",
      "[100  50   2   0   0   0   0   0   0   0]\n",
      "[100  70  50  23  16  15  15  11   9   9]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  17   0   0   0   0   0   0   0   0]\n",
      "[100   4   0   0   0   0   0   0   0   0]\n",
      "[100  14   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   7   0   0   0   0   0   0   0   0]\n",
      "[100  12   3   3   2   1   1   1   1   1]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  63  48  41  29  27  25  25  24  21]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  15   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100  11   6   1   1   0   0   0   0   0]\n",
      "[100  41   9   2   1   1   1   1   0   0]\n",
      "[100  91  85  39  18  15   9   7   7   5]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   5   3   2   1   0   0   0   0   0]\n",
      "[100  78  59  46  19  16  12  11  11   3]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   3   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100   3   0   0   0   0   0   0   0   0]\n",
      "[100   5   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100  20   2   1   0   0   0   0   0   0]\n",
      "[100  68  11   2   1   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  11   1   0   0   0   0   0   0   0]\n",
      "[100  11   2   0   0   0   0   0   0   0]\n",
      "[100  17   3   2   0   0   0   0   0   0]\n",
      "[100  22  19   3   0   0   0   0   0   0]\n",
      "[100   9   3   1   0   0   0   0   0   0]\n",
      "[100   7   4   2   1   0   0   0   0   0]\n",
      "[100  85  73  13   4   4   1   1   0   0]\n",
      "[100  34  10   7   7   2   1   1   0   0]\n",
      "[100   6   3   1   0   0   0   0   0   0]\n",
      "[100  11   2   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   7   5   5   4   3   3   2   2   2]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   5   0   0   0   0   0   0   0   0]\n",
      "[100   3   0   0   0   0   0   0   0   0]\n",
      "[100   3   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100   6   0   0   0   0   0   0   0   0]\n",
      "[100   7   0   0   0   0   0   0   0   0]\n",
      "[100   5   4   2   1   1   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  97  51  40  31  15  13   4   3   3]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   2   2   1   0   0   0   0   0   0]\n",
      "[100  63  45  45   7   2   2   1   1   1]\n",
      "[100  69  55  24  23  22  21  16  16   7]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   3   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100  62   0   0   0   0   0   0   0   0]\n",
      "[100   9   0   0   0   0   0   0   0   0]\n",
      "[100   3   0   0   0   0   0   0   0   0]\n",
      "[100   9   4   1   1   1   1   1   0   0]\n",
      "[100  11  10   6   5   4   4   3   1   1]\n",
      "[100  32  30  25  16  10   9   8   7   6]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   6   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100   6   3   0   0   0   0   0   0   0]\n",
      "[100   1   1   0   0   0   0   0   0   0]\n",
      "[100  42  14  12   9   3   3   2   2   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   9   0   0   0   0   0   0   0   0]\n",
      "[100   4   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   3   0   0   0   0   0   0   0   0]\n",
      "[100   4   2   1   1   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  55  20  17  10   6   5   4   3   3]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   3   0   0   0   0   0   0   0   0]\n",
      "[100  27   1   0   0   0   0   0   0   0]\n",
      "[100  19   2   0   0   0   0   0   0   0]\n",
      "[100  87  82  51  14  10   5   5   4   3]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100  10   0   0   0   0   0   0   0   0]\n",
      "[100  25   0   0   0   0   0   0   0   0]\n",
      "[100  78  14  11   6   6   3   3   2   1]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   6   0   0   0   0   0   0   0   0]\n",
      "[100   8   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   6   0   0   0   0   0   0   0   0]\n",
      "[100  26   0   0   0   0   0   0   0   0]\n",
      "[100  81  39  27  26  22   9   6   2   2]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  89  43  33  30  26  25  25  22  11]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   4   0   0   0   0   0   0   0   0]\n",
      "[100  62   2   1   0   0   0   0   0   0]\n",
      "[100  65  55  41  23  17   8   7   6   5]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   3   0   0   0   0   0   0   0   0]\n",
      "[100   4   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100  29   0   0   0   0   0   0   0   0]\n",
      "[100  78   0   0   0   0   0   0   0   0]\n",
      "[100  61  40  13  12  12   6   6   5   2]\n",
      "[100   3   1   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  64  32  26  24  12  10   7   7   7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   3   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   9   2   0   0   0   0   0   0   0]\n",
      "[100  40   3   0   0   0   0   0   0   0]\n",
      "[100  66  47  43  42  15  13   9   8   6]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   3   0   0   0   0   0   0   0   0]\n",
      "[100   7   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   8   0   0   0   0   0   0   0   0]\n",
      "[100  57   0   0   0   0   0   0   0   0]\n",
      "[100  68  54  47  33  20   7   4   4   3]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100  16   0   0   0   0   0   0   0   0]\n",
      "[100  96   0   0   0   0   0   0   0   0]\n",
      "[100   3   0   0   0   0   0   0   0   0]\n",
      "[100  10   0   0   0   0   0   0   0   0]\n",
      "[100   8   6   3   2   2   1   1   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100  45  24  21  16  14  13  10  10   5]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   3   0   0   0   0   0   0   0   0]\n",
      "[100   5   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   5   3   3   2   2   1   1   1   1]\n",
      "[100   2   1   0   0   0   0   0   0   0]\n",
      "[100   2   1   0   0   0   0   0   0   0]\n",
      "[100  17  16  14   8   7   6   5   4   4]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  10   0   0   0   0   0   0   0   0]\n",
      "[100  35   1   0   0   0   0   0   0   0]\n",
      "[100   3   1   1   0   0   0   0   0   0]\n",
      "[100  31  25  21  15  10   6   4   2   2]\n",
      "[100  95  95  80  39  21  17  15   6   3]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  35   0   0   0   0   0   0   0   0]\n",
      "[100  25   1   1   1   0   0   0   0   0]\n",
      "[100  22   2   1   0   0   0   0   0   0]\n",
      "[100  30   9   6   2   1   1   0   0   0]\n",
      "[100  49  16   7   3   2   1   1   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   7   0   0   0   0   0   0   0   0]\n",
      "[100  30   0   0   0   0   0   0   0   0]\n",
      "[100  66   1   0   0   0   0   0   0   0]\n",
      "[100  27   0   0   0   0   0   0   0   0]\n",
      "[100  17   0   0   0   0   0   0   0   0]\n",
      "[100   9   1   0   0   0   0   0   0   0]\n",
      "[100  35   2   0   0   0   0   0   0   0]\n",
      "[100  37   0   0   0   0   0   0   0   0]\n",
      "[100  61   0   0   0   0   0   0   0   0]\n",
      "[100  58   3   1   0   0   0   0   0   0]\n",
      "[100  82  49  41  33  27  23  15  11  10]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  50   0   0   0   0   0   0   0   0]\n",
      "[100   7   0   0   0   0   0   0   0   0]\n",
      "[100   7   0   0   0   0   0   0   0   0]\n",
      "[100  86   0   0   0   0   0   0   0   0]\n",
      "[100   8   0   0   0   0   0   0   0   0]\n",
      "[100   6   0   0   0   0   0   0   0   0]\n",
      "[100   8   0   0   0   0   0   0   0   0]\n",
      "[100  47   0   0   0   0   0   0   0   0]\n",
      "[100   4   3   2   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  75  55  34  33  32  16  12   5   4]\n",
      "[100  91  15   9   7   7   4   4   2   2]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  69   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100  13   0   0   0   0   0   0   0   0]\n",
      "[100  39   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   9   0   0   0   0   0   0   0   0]\n",
      "[100   9   0   0   0   0   0   0   0   0]\n",
      "[100  47   1   0   0   0   0   0   0   0]\n",
      "[100  16   9   3   2   1   0   0   0   0]\n",
      "[100   8   6   4   3   3   2   1   1   0]\n",
      "[100   2   1   1   0   0   0   0   0   0]\n",
      "[100  90  87  41  25  15  11   5   4   4]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  35   0   0   0   0   0   0   0   0]\n",
      "[100   4   0   0   0   0   0   0   0   0]\n",
      "[100   4   0   0   0   0   0   0   0   0]\n",
      "[100  13   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100   3   0   0   0   0   0   0   0   0]\n",
      "[100  39   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100  60  21   6   4   2   2   2   1   1]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  13   0   0   0   0   0   0   0   0]\n",
      "[100  15   0   0   0   0   0   0   0   0]\n",
      "[100  17   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   3   0   0   0   0   0   0   0   0]\n",
      "[100   3   0   0   0   0   0   0   0   0]\n",
      "[100  17   2   1   0   0   0   0   0   0]\n",
      "[100  63   0   0   0   0   0   0   0   0]\n",
      "[100  93  89  56  45  13  10   7   4   4]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   8   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   4   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100   4   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   3   0   0   0   0   0   0   0   0]\n",
      "[100   3   0   0   0   0   0   0   0   0]\n",
      "[100   3   3   1   1   1   0   0   0   0]\n",
      "[100  15   3   2   2   0   0   0   0   0]\n",
      "[100  25   9   8   6   5   5   4   3   2]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  15   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  42  41  40  31  21  18  15  10   8]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  90   1   0   0   0   0   0   0   0]\n",
      "[100  35   7   5   5   5   4   1   1   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   8   3   2   1   1   1   0   0   0]\n",
      "[100  55  16   5   5   4   2   1   1   1]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100  55  35  29  15  14   9   8   5   1]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  77   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  10   5   4   3   1   0   0   0   0]\n",
      "[100   2   1   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   4   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  92  36  28  25  13   8   7   5   4]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  34   0   0   0   0   0   0   0   0]\n",
      "[100  84  54  28  25  17  16  11   3   2]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  52   0   0   0   0   0   0   0   0]\n",
      "[100  83   0   0   0   0   0   0   0   0]\n",
      "[100   4   1   0   0   0   0   0   0   0]\n",
      "[100  65   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  12   6   1   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   1   1   0   0   0   0   0   0   0]\n",
      "[100  73  45  42  31  25  14  14  14  12]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  84   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  46  32  31  23  16  15  12  11   7]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  77   0   0   0   0   0   0   0   0]\n",
      "[100  21   2   1   1   0   0   0   0   0]\n",
      "[100   7   0   0   0   0   0   0   0   0]\n",
      "[100   9   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100  22   1   1   0   0   0   0   0   0]\n",
      "[100  36   0   0   0   0   0   0   0   0]\n",
      "[100  19   3   0   0   0   0   0   0   0]\n",
      "[100  72  19  17  13  12   8   7   5   2]\n",
      "[100  25   7   7   2   2   2   1   1   1]\n",
      "[100  30   1   1   0   0   0   0   0   0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100   9   6   4   0   0   0   0   0   0]\n",
      "[100  27  24  17  15  10   6   3   1   0]\n",
      "[100  13   5   1   0   0   0   0   0   0]\n",
      "[100  22   0   0   0   0   0   0   0   0]\n",
      "[100  12   3   1   1   0   0   0   0   0]\n",
      "[100  12   9   2   1   0   0   0   0   0]\n",
      "[100   9   3   1   0   0   0   0   0   0]\n",
      "[100  10   3   0   0   0   0   0   0   0]\n",
      "[100   7   3   0   0   0   0   0   0   0]\n",
      "[100   6   4   1   0   0   0   0   0   0]\n",
      "[100   8   5   1   0   0   0   0   0   0]\n",
      "[100  17   6   1   1   0   0   0   0   0]\n",
      "[100  50  23   9   6   5   3   2   2   2]\n",
      "[100  49  15  12  10   7   5   3   2   1]\n",
      "[100  62  45  38  33  31  23  20  14  14]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  46   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  98  33  23  17  14  10  10   2   2]\n",
      "[100  97  48  38  33  13   6   6   5   5]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100  34   2   0   0   0   0   0   0   0]\n",
      "[100   3   2   1   1   1   0   0   0   0]\n",
      "[100  15   4   2   2   1   0   0   0   0]\n",
      "[100  16   9   6   3   2   1   1   1   0]\n",
      "[100   7   3   3   2   2   2   1   1   0]\n",
      "[100   2   1   0   0   0   0   0   0   0]\n",
      "[100  73   2   0   0   0   0   0   0   0]\n",
      "[100  25   7   3   2   2   1   1   1   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  93   0   0   0   0   0   0   0   0]\n",
      "[100  25  12  10   2   1   0   0   0   0]\n",
      "[100   7   4   1   0   0   0   0   0   0]\n",
      "[100  19   1   0   0   0   0   0   0   0]\n",
      "[100   7   0   0   0   0   0   0   0   0]\n",
      "[100  10   2   1   1   0   0   0   0   0]\n",
      "[100  60   7   1   1   0   0   0   0   0]\n",
      "[100   7   2   0   0   0   0   0   0   0]\n",
      "[100   3   0   0   0   0   0   0   0   0]\n",
      "[100  85  11   6   6   4   3   2   1   1]\n",
      "[100  24   6   5   3   2   0   0   0   0]\n",
      "[100  29  15  13  11  11   7   6   5   3]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  42   0   0   0   0   0   0   0   0]\n",
      "[100   4   0   0   0   0   0   0   0   0]\n",
      "[100   8   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100   2   0   0   0   0   0   0   0   0]\n",
      "[100   3   0   0   0   0   0   0   0   0]\n",
      "[100   5   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  93  20  18  17  13  11  11   9   8]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  12   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   3   0   0   0   0   0   0   0   0]\n",
      "[100   4   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100  11   4   3   1   0   0   0   0   0]\n",
      "[100  24  22  10   4   1   1   0   0   0]\n",
      "[100  28  28  22  18  13   6   4   3   2]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  42   3   0   0   0   0   0   0   0]\n",
      "[100  20   3   0   0   0   0   0   0   0]\n",
      "[100   3   0   0   0   0   0   0   0   0]\n",
      "[100  55  13  11   9   4   4   3   2   1]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   8   3   2   1   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  15   0   0   0   0   0   0   0   0]\n",
      "[100   3   0   0   0   0   0   0   0   0]\n",
      "[100  47  22  18  10   5   4   4   4   3]\n",
      "[100  50   0   0   0   0   0   0   0   0]\n",
      "[100  36  35  35  33  25  23  15  14  11]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   5   0   0   0   0   0   0   0   0]\n",
      "[100   7   0   0   0   0   0   0   0   0]\n",
      "[100   8   0   0   0   0   0   0   0   0]\n",
      "[100  44   0   0   0   0   0   0   0   0]\n",
      "[100  24  20  18  15   8   8   4   4   4]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  26   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   1   1   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  25  24  23  23  17  13  12   9   6]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   2   1   0   0   0   0   0   0   0]\n",
      "[100  23  13  10  10   8   7   7   7   7]\n",
      "[100  57  52  30  24  23  22  20  20  16]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100  69   0   0   0   0   0   0   0   0]\n",
      "[100  81   1   0   0   0   0   0   0   0]\n",
      "[100   7   1   0   0   0   0   0   0   0]\n",
      "[100  27   0   0   0   0   0   0   0   0]\n",
      "[100   3   1   0   0   0   0   0   0   0]\n",
      "[100  50   3   1   1   1   1   0   0   0]\n",
      "[100  11   0   0   0   0   0   0   0   0]\n",
      "[100   7   0   0   0   0   0   0   0   0]\n",
      "[100  43   0   0   0   0   0   0   0   0]\n",
      "[100  41  23  11   9   7   5   3   3   2]\n",
      "[100   8   4   4   3   1   1   1   0   0]\n",
      "[100  79   0   0   0   0   0   0   0   0]\n",
      "[100  34   0   0   0   0   0   0   0   0]\n",
      "[100  82   0   0   0   0   0   0   0   0]\n",
      "[100   9   4   2   1   1   1   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   9   8   5   5   4   4   2   0   0]\n",
      "[100   9   4   2   1   1   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  35   4   2   1   1   1   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  64   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  26  16  13  13  12  11   5   4   2]\n",
      "[100  44  27  13  12  12   7   4   4   1]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  75   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100  20   0   0   0   0   0   0   0   0]\n",
      "[100   6   0   0   0   0   0   0   0   0]\n",
      "[100   5   3   2   2   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  13   7   3   2   2   2   1   1   1]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  91   0   0   0   0   0   0   0   0]\n",
      "[100  28   1   0   0   0   0   0   0   0]\n",
      "[100  14  11   3   3   2   1   0   0   0]\n",
      "[100  41   4   1   1   0   0   0   0   0]\n",
      "[100  28  24   8   5   3   3   2   1   1]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100  84   7   1   1   0   0   0   0   0]\n",
      "[100  40   7   6   5   4   1   1   1   0]\n",
      "[100  10   2   0   0   0   0   0   0   0]\n",
      "[100  19   2   0   0   0   0   0   0   0]\n",
      "[100   5   0   0   0   0   0   0   0   0]\n",
      "[100  40   8   4   3   1   1   1   1   0]\n",
      "[100  20   3   0   0   0   0   0   0   0]\n",
      "[100   5   5   0   0   0   0   0   0   0]\n",
      "[100  60   2   1   0   0   0   0   0   0]\n",
      "[100  15  13   7   5   5   4   3   3   3]\n",
      "[100  55  40  22  21  14  12  12  11  11]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   6   0   0   0   0   0   0   0   0]\n",
      "[100   8   3   1   0   0   0   0   0   0]\n",
      "[100   5   0   0   0   0   0   0   0   0]\n",
      "[100  14   2   2   1   1   1   0   0   0]\n",
      "[100   1   1   0   0   0   0   0   0   0]\n",
      "[100   6   3   2   1   0   0   0   0   0]\n",
      "[100   7   4   3   2   1   1   1   1   1]\n",
      "[100   4   3   3   3   2   2   1   1   1]\n",
      "[100   5   4   2   1   1   1   1   0   0]\n",
      "[100  23  11   7   6   5   5   4   3   3]\n",
      "[100   5   1   1   0   0   0   0   0   0]\n",
      "[100  13   9   3   3   1   1   1   1   1]\n",
      "[100  20  11   9   3   3   3   1   1   1]\n",
      "[100   5   2   1   0   0   0   0   0   0]\n",
      "[100   6   3   2   1   0   0   0   0   0]\n",
      "[100  22   2   2   1   1   1   0   0   0]\n",
      "[100   7   3   2   1   0   0   0   0   0]\n",
      "[100  20  11   5   1   0   0   0   0   0]\n",
      "[100  11   4   3   3   2   2   1   1   0]\n",
      "[100   3   1   1   0   0   0   0   0   0]\n",
      "[100   5   3   1   1   1   0   0   0   0]\n",
      "[100   5   0   0   0   0   0   0   0   0]\n",
      "[100   9   2   2   0   0   0   0   0   0]\n",
      "[100   2   1   1   1   1   1   0   0   0]\n",
      "[100  16  15   4   4   3   1   1   1   1]\n",
      "[100   2   2   1   1   0   0   0   0   0]\n",
      "[100  17   7   4   3   3   2   2   1   1]\n",
      "[100  14   8   8   6   5   2   1   1   1]\n",
      "[100   9   7   5   2   2   1   0   0   0]\n",
      "[100   6   4   3   2   1   1   1   0   0]\n",
      "[100   7   4   4   4   3   2   2   1   0]\n",
      "[100   9   7   6   4   3   2   2   1   1]\n",
      "[100   8   7   6   4   3   2   1   1   1]\n",
      "[100  72  25  22  19  15   7   6   4   4]\n",
      "[100   7   3   1   1   0   0   0   0   0]\n",
      "[100  28  23   8   7   7   7   6   5   4]\n",
      "[100   4   3   2   1   1   1   1   1   1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100  11   7   6   4   4   4   4   3   3]\n",
      "[100   4   3   2   2   1   1   1   1   1]\n",
      "[100  25  10   9   7   6   6   4   4   3]\n",
      "[100   4   3   3   3   3   2   1   1   1]\n",
      "[100  16  16   8   3   2   2   1   1   1]\n",
      "[100  25  23  18  15  10   9   6   6   6]\n",
      "[100  99  85  73  41  38  30  22  15  12]\n",
      "[100   2   1   1   1   1   1   0   0   0]\n",
      "[100  39   3   2   1   0   0   0   0   0]\n",
      "[100  12  11   7   6   6   4   4   3   2]\n",
      "[100   9   7   7   7   5   4   3   3   2]\n",
      "[100   6   5   3   3   3   2   2   2   2]\n",
      "[100  18   9   6   5   5   4   3   3   2]\n",
      "[100   9   7   5   4   3   3   2   2   1]\n",
      "[100  16   2   2   2   2   2   2   1   1]\n",
      "[100  15  15  15  14  11  10   8   7   5]\n",
      "[100   6   5   4   3   2   2   2   1   1]\n",
      "[100  18  17  11   8   4   3   3   2   2]\n",
      "[100  12  12   8   4   3   2   2   1   1]\n",
      "[100  22  20   7   4   3   2   2   2   2]\n",
      "[100   9   4   3   3   2   2   2   1   1]\n",
      "[100  12   7   6   5   3   2   2   2   2]\n",
      "[100  46  33  23  22  20  16  15   9   8]\n",
      "[100   5   4   2   2   1   1   1   1   1]\n",
      "[100  64  31  16  14  11  10   9   6   6]\n",
      "[100  14  10   9   8   8   4   2   2   2]\n",
      "[100  17  12   8   4   2   2   1   1   1]\n",
      "[100   7   3   2   2   2   1   1   1   1]\n",
      "[100  60  55  49  34  30  21  17  15  12]\n",
      "[100  23  12  12   9   7   6   6   6   5]\n",
      "[100  76  43  37  34  24  14  11  10   9]\n",
      "[100  10   9   6   5   3   2   2   2   1]\n",
      "[100  24  19  10   7   5   4   3   3   3]\n",
      "[100   6   4   3   3   2   2   1   1   1]\n",
      "[100  38  31  24  21  12  11  10   9   9]\n",
      "[100  35  28  23  20  19  16  16  15  14]\n",
      "[100   7   5   4   3   3   3   2   2   1]\n",
      "[100   4   3   2   1   1   1   1   1   1]\n",
      "[100   3   3   2   1   1   1   0   0   0]\n",
      "[100  95  24  20  13   9   7   5   5   4]\n",
      "[100   7   6   4   3   2   2   2   2   1]\n",
      "[100  36  25  22  16  14  13  13  12   9]\n",
      "[100  23  19   8   6   5   4   4   3   2]\n",
      "[100  13  11   5   5   5   4   4   4   4]\n",
      "[100   7   7   6   5   5   4   4   3   3]\n",
      "[100  17   8   8   7   4   3   2   2   1]\n",
      "[100  61  29  24  24  18  16   9   9   8]\n",
      "[100  15  13   6   5   3   3   2   2   2]\n",
      "[100  34  24   9   7   6   5   5   3   3]\n",
      "[100  25  24   9   8   5   5   5   4   2]\n",
      "[100  22  21  10   9   8   7   7   5   5]\n",
      "[100  18  15  10   8   5   4   4   3   3]\n",
      "[100  21  17   7   6   5   5   4   4   3]\n",
      "[100  13   6   5   4   3   3   2   1   1]\n",
      "[100  10   2   2   1   1   1   1   1   0]\n",
      "[100  20  12  11  10   9   7   5   4   3]\n",
      "[100   5   4   2   2   2   1   1   1   1]\n",
      "[100   3   3   2   1   1   1   1   0   0]\n",
      "[100  19  12  11  10   9   5   4   3   3]\n",
      "[100  13  11  11   8   6   6   5   3   3]\n",
      "[100  16  11  10  10   8   3   2   1   1]\n",
      "[100  53  44  30  19  16  12  10   8   5]\n",
      "[100  14  11   7   6   5   5   3   3   3]\n",
      "[100  95  12  12  11   9   6   5   5   4]\n",
      "[100  60   6   5   5   4   4   4   4   3]\n",
      "[100  38   6   4   4   4   3   3   3   2]\n",
      "[100  31  10   6   4   4   3   3   2   1]\n",
      "[100   6   5   2   1   1   1   0   0   0]\n",
      "[100  75  11  10  10   9   7   7   5   4]\n",
      "[100  16  10   7   6   5   1   1   1   1]\n",
      "[100  48   9   7   5   2   2   2   1   1]\n",
      "[100  10   4   3   2   2   1   1   1   0]\n",
      "[100   7   5   2   1   1   1   0   0   0]\n",
      "[100  10   4   4   2   1   1   1   1   0]\n",
      "[100   7   7   7   5   4   4   3   2   2]\n",
      "[100  22  18  13   7   4   3   3   3   3]\n",
      "[100   4   3   3   1   1   1   0   0   0]\n",
      "[100  44  43  38  11   9   8   7   6   4]\n",
      "[100  12   6   5   4   4   3   3   2   2]\n",
      "[100  17  11   9   7   5   5   4   2   2]\n",
      "[100  10   9   9   8   7   6   5   4   2]\n",
      "[100  24  23  21  15  14  12  12   7   5]\n",
      "[100  64  21   8   7   6   4   3   2   1]\n",
      "[100  68  30  21  20  17  11  11   7   6]\n",
      "[100  63  19  17  11   9   7   6   6   6]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  16   3   2   1   0   0   0   0   0]\n",
      "[100  16  15  12   4   3   2   2   2   1]\n",
      "[100  39  28  22   9   8   6   5   5   4]\n",
      "[100  21  14  11   7   6   5   3   2   1]\n",
      "[100   9   9   4   3   2   1   1   1   1]\n",
      "[100   6   4   4   2   1   1   1   1   1]\n",
      "[100   6   2   2   2   1   0   0   0   0]\n",
      "[100   3   3   2   2   0   0   0   0   0]\n",
      "[100   2   1   1   0   0   0   0   0   0]\n",
      "[100   3   2   1   1   1   1   0   0   0]\n",
      "[100  60  55  49  47  45  20  15  14  11]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   8   1   1   1   1   1   0   0   0]\n",
      "[100  28  11   8   4   3   1   1   1   1]\n",
      "[100  20  16  15  11   9   6   5   4   3]\n",
      "[100   4   1   1   1   1   0   0   0   0]\n",
      "[100  12   8   3   3   2   2   1   1   1]\n",
      "[100  42  29  16  13  13   5   5   4   3]\n",
      "[100   7   4   4   3   2   2   1   1   1]\n",
      "[100   4   3   3   3   2   2   2   1   1]\n",
      "[100  31  15  12  10  10   7   3   3   2]\n",
      "[100  59  31  16  15   9   4   3   2   1]\n",
      "[100  53  39  34  10   9   8   7   7   6]\n",
      "[100  17   9   8   6   2   2   2   2   1]\n",
      "[100  41  37  31  26  19  16   2   1   1]\n",
      "[100  68  29  18  17  10   6   4   3   2]\n",
      "[100  94  51  48  45  41  33  18  11   9]\n",
      "[100  12   8   6   3   2   1   1   0   0]\n",
      "[100  90   3   3   3   2   2   1   1   1]\n",
      "[100  36  27  22  19  13  12   7   6   3]\n",
      "[100   3   1   1   1   1   0   0   0   0]\n",
      "[100  15  12   6   6   4   3   2   2   2]\n",
      "[100  75   5   3   2   2   1   1   0   0]\n",
      "[100  65  49  42  13  11  11  10   9   7]\n",
      "[100  30  14   8   7   5   4   3   2   2]\n",
      "[100  16  12   7   1   1   1   1   1   0]\n",
      "[100   7   4   2   1   1   1   0   0   0]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  32   4   2   2   1   0   0   0   0]\n",
      "[100  36   7   5   2   2   2   1   1   1]\n",
      "[100  39  22  18  12  11   8   7   4   3]\n",
      "[100   2   1   0   0   0   0   0   0   0]\n",
      "[100  84   1   0   0   0   0   0   0   0]\n",
      "[100  56   5   3   2   1   1   1   0   0]\n",
      "[100  21   9   4   3   2   2   2   2   1]\n",
      "[100  12   6   4   2   2   2   1   0   0]\n",
      "[100  69  56  34  23  16   9   8   7   7]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100  63   1   1   0   0   0   0   0   0]\n",
      "[100  43  25  12  11  11  10   5   4   3]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  89   1   1   0   0   0   0   0   0]\n",
      "[100   9   7   6   5   2   2   2   2   1]\n",
      "[100   9   2   2   2   1   1   1   0   0]\n",
      "[100  23   2   1   0   0   0   0   0   0]\n",
      "[100  62  43  38  32  25  22  13  10   9]\n",
      "[100   5   4   3   2   2   2   1   1   0]\n",
      "[100  54   1   0   0   0   0   0   0   0]\n",
      "[100  53   1   0   0   0   0   0   0   0]\n",
      "[100  15   1   1   1   0   0   0   0   0]\n",
      "[100  49   0   0   0   0   0   0   0   0]\n",
      "[100  78  52  31  31  27  27  23  20  19]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  48   0   0   0   0   0   0   0   0]\n",
      "[100   2   2   1   1   1   0   0   0   0]\n",
      "[100   3   1   1   1   0   0   0   0   0]\n",
      "[100   2   1   1   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   8   6   4   4   4   2   1   1   1]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100   1   1   1   1   0   0   0   0   0]\n",
      "[100   4   3   2   1   0   0   0   0   0]\n",
      "[100  36  27  26   7   6   5   3   1   0]\n",
      "[100  24   2   1   1   0   0   0   0   0]\n",
      "[100  42  40  36  24  22   8   6   4   4]\n",
      "[100   2   1   1   1   0   0   0   0   0]\n",
      "[100  83   0   0   0   0   0   0   0   0]\n",
      "[100  48  43  28  28  23  23  11  10   7]\n",
      "[100   1   1   0   0   0   0   0   0   0]\n",
      "[100  40   2   0   0   0   0   0   0   0]\n",
      "[100  90  52  47  27  22  13  12   9   4]\n",
      "[100   2   1   0   0   0   0   0   0   0]\n",
      "[100  35   0   0   0   0   0   0   0   0]\n",
      "[100  65   2   2   0   0   0   0   0   0]\n",
      "[100  67  44  42  20  15  12  11   7   4]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  75   0   0   0   0   0   0   0   0]\n",
      "[100   1   1   0   0   0   0   0   0   0]\n",
      "[100   5   5   4   3   2   2   1   1   1]\n",
      "[100   1   1   0   0   0   0   0   0   0]\n",
      "[100  51   0   0   0   0   0   0   0   0]\n",
      "[100  86   3   0   0   0   0   0   0   0]\n",
      "[100  79   1   0   0   0   0   0   0   0]\n",
      "[100  50  38  22  21  13  13  11  11   3]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  46   0   0   0   0   0   0   0   0]\n",
      "[100   6   0   0   0   0   0   0   0   0]\n",
      "[100  13   2   2   1   1   0   0   0   0]\n",
      "[100   6   5   1   1   0   0   0   0   0]\n",
      "[100  63   4   1   0   0   0   0   0   0]\n",
      "[100  51   7   1   1   0   0   0   0   0]\n",
      "[100  53   9   4   3   1   1   0   0   0]\n",
      "[100  47  44  28  27  17  11   8   8   3]\n",
      "[100   1   0   0   0   0   0   0   0   0]\n",
      "[100  17   0   0   0   0   0   0   0   0]\n",
      "[100  71   0   0   0   0   0   0   0   0]\n",
      "[100  14   3   3   3   2   2   1   1   1]\n",
      "[100  20   7   6   4   4   4   3   1   1]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  66   0   0   0   0   0   0   0   0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100  12   1   0   0   0   0   0   0   0]\n",
      "[100  31  27  22  22  18  18  10   6   5]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  44   0   0   0   0   0   0   0   0]\n",
      "[100  57   3   3   2   1   1   0   0   0]\n",
      "[100  45   1   0   0   0   0   0   0   0]\n",
      "[100  61  47  43  28  18  14  11   9   9]\n",
      "[100   0   0   0   0   0   0   0   0   0]\n",
      "[100  18   0   0   0   0   0   0   0   0]\n",
      "[100  22   1   1   1   0   0   0   0   0]\n",
      "[100  96   3   2   2   1   1   0   0   0]\n",
      "[100   8   6   1   1   1   0   0   0   0]\n",
      "[100   9   4   2   2   1   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "ch = range(1300000)\n",
    "ch = 1300\n",
    "X_in = np.reshape(dataX[ch] ,(1, 50, 128))\n",
    "#X_in = np.reshape(dataX[np.random.choice(ch)] ,(1, 50, 128))\n",
    "#X_in = np.reshape([to_categorical(np.random.choice(range(128)), num_classes = 128) if i % 2 == 0 else to_categorical(1, num_classes = 128) for i in range(50)],(1, 50, 128))\n",
    "#X_in = np.reshape([to_categorical(i, num_classes = 128) if i % 2 == 0 else to_categorical(1, num_classes = 128) for i in range(34,84)],(1, 50, 128))\n",
    "new_track = [np.argmax(x) for x in X_in[0]] + [120, 1, 1, 1]\n",
    "p = 2\n",
    "previous = 1\n",
    "for i in range(1200):\n",
    "    prediction = model.predict(X_in)\n",
    "    print(np.array(prediction[0][sorted(range(128), key = lambda x:prediction[0,x], reverse = True)] / np.max(prediction, axis = 1) * 100, \"int32\")[:10])\n",
    "    '''\n",
    "    if p!= 1:\n",
    "        prediction[0,p] /= 10.0 \n",
    "    else:\n",
    "        prediction[0,1] *= 2.0\n",
    "    '''\n",
    "    prediction[0,previous] = 0.0\n",
    "    p = take_one(prediction)\n",
    "    if (p != 1 and p != 0):\n",
    "        previous = p\n",
    "    #print(p)\n",
    "    #previous_note = p\n",
    "    new_track.append(p)\n",
    "    X_in = list(X_in[0])[1:]\n",
    "    p_cat = to_categorical(p, num_classes = 128)\n",
    "    \n",
    "    X_in.append(p_cat)\n",
    "    \n",
    "    X_in = np.reshape(X_in,(1, 50, 128))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "create_track_new(new_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from midiutil import MIDIFile\n",
    "\n",
    "def create_track_new(seq):\n",
    "    \n",
    "    grain = 16\n",
    "    one_tact = 16\n",
    "    tacts_per_min = 75\n",
    "    tempo    = one_tact * tacts_per_min  # In BPM\n",
    "    volume   = 100  # 0-127, as per the MIDI standard\n",
    "    track    = 0\n",
    "    channel  = 0\n",
    "    time     = 0# to learn\n",
    "    \n",
    "    MyMIDI = MIDIFile(1)\n",
    "    MyMIDI.addTempo(track, time, tempo)\n",
    "    \n",
    "    \n",
    "    i = 0\n",
    "    tick_count = 0\n",
    "    while(i < len(seq)):\n",
    "        \n",
    "        if i == len(seq) - 1:\n",
    "            MyMIDI.addNote(track, channel, seq[i], tick_count, one_tact / grain, volume)\n",
    "            tick_count = tick_count + one_tact / grain\n",
    "            i = i + 1\n",
    "        else:\n",
    "            if seq[i + 1] != 1:\n",
    "                MyMIDI.addNote(track, channel, seq[i], tick_count, one_tact / grain, volume)\n",
    "                tick_count = tick_count + one_tact / grain \n",
    "                i = i + 1\n",
    "            else:\n",
    "                i_prev = i\n",
    "                i = i + 1\n",
    "                while(seq[i] == 1 and i < len(seq) - 1):\n",
    "                    i += 1\n",
    "                \n",
    "                    \n",
    "                #i = i + 1#??\n",
    "\n",
    "                d = i - i_prev\n",
    "                \n",
    "                MyMIDI.addNote(track, channel, seq[i_prev], tick_count, d * one_tact / grain, volume)\n",
    "                tick_count = tick_count + d * one_tact / grain \n",
    "                \n",
    "                if (i == len(seq)):\n",
    "                        break\n",
    "                \n",
    "                if (seq[i] == 0):\n",
    "                    i_prev = i\n",
    "                    if (i == len(seq) - 1):\n",
    "                        break\n",
    "                    while(seq[i] == 1 and i < len(seq) - 1):\n",
    "                        i = i + 1\n",
    "                        tick_count = tick_count + one_tact / grain \n",
    "                        \n",
    "\n",
    "    \n",
    "    print(track)\n",
    "\n",
    "    with open(\"zzz3.mid\", \"wb\") as output_file:\n",
    "        MyMIDI.writeFile(output_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = take_one(model.predict(X_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"my_impr-0306.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(new_track, fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(to_categorical(p, num_classes = 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524002"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
