{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout,Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "# load ascii text and covert to lowercase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load( open( \"nobass.txt\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "615"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(sum(data,[])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "seq_length = 8\n",
    "\n",
    "for seq in data: \n",
    "    n_chars = len(seq)\n",
    "    for i in range(0, n_chars - seq_length, 1):\n",
    "        seq_in = seq[i:i + seq_length]\n",
    "        seq_out = seq[i + seq_length]\n",
    "        dataX.append(seq_in)\n",
    "        dataY.append(seq_out)\n",
    "n_patterns = len(dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:100000]\n",
    "Y = dataY[:100000]\n",
    "X = X / float(688)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(Y, 688)\n",
    "# define the LSTM model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "opt = keras.optimizers.Adam()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100000/100000 [==============================] - 23s 227us/step - loss: 3.5024\n",
      "Epoch 2/30\n",
      "100000/100000 [==============================] - 23s 228us/step - loss: 3.4906\n",
      "Epoch 3/30\n",
      "100000/100000 [==============================] - 23s 233us/step - loss: 3.4802\n",
      "Epoch 4/30\n",
      "100000/100000 [==============================] - 24s 238us/step - loss: 3.4694\n",
      "Epoch 5/30\n",
      "100000/100000 [==============================] - 24s 237us/step - loss: 3.4589\n",
      "Epoch 6/30\n",
      "100000/100000 [==============================] - 24s 237us/step - loss: 3.4506\n",
      "Epoch 7/30\n",
      "100000/100000 [==============================] - 24s 237us/step - loss: 3.4443\n",
      "Epoch 8/30\n",
      "100000/100000 [==============================] - 24s 238us/step - loss: 3.4313\n",
      "Epoch 9/30\n",
      "100000/100000 [==============================] - 24s 238us/step - loss: 3.4223\n",
      "Epoch 10/30\n",
      "100000/100000 [==============================] - 24s 239us/step - loss: 3.4142\n",
      "Epoch 11/30\n",
      "100000/100000 [==============================] - 24s 238us/step - loss: 3.4090\n",
      "Epoch 12/30\n",
      "100000/100000 [==============================] - 24s 237us/step - loss: 3.3923\n",
      "Epoch 13/30\n",
      "100000/100000 [==============================] - 24s 237us/step - loss: 3.3923\n",
      "Epoch 14/30\n",
      "100000/100000 [==============================] - 24s 239us/step - loss: 3.3805\n",
      "Epoch 15/30\n",
      "100000/100000 [==============================] - 24s 237us/step - loss: 3.3742\n",
      "Epoch 16/30\n",
      "100000/100000 [==============================] - 24s 236us/step - loss: 3.3659\n",
      "Epoch 17/30\n",
      "100000/100000 [==============================] - 24s 237us/step - loss: 3.3566\n",
      "Epoch 18/30\n",
      "100000/100000 [==============================] - 24s 238us/step - loss: 3.3512\n",
      "Epoch 19/30\n",
      "100000/100000 [==============================] - 24s 238us/step - loss: 3.3421\n",
      "Epoch 20/30\n",
      "100000/100000 [==============================] - 24s 240us/step - loss: 3.3343\n",
      "Epoch 21/30\n",
      "100000/100000 [==============================] - 24s 240us/step - loss: 3.3291\n",
      "Epoch 22/30\n",
      "100000/100000 [==============================] - 24s 237us/step - loss: 3.3196\n",
      "Epoch 23/30\n",
      "100000/100000 [==============================] - 24s 238us/step - loss: 3.3130\n",
      "Epoch 24/30\n",
      "100000/100000 [==============================] - 24s 239us/step - loss: 3.3058\n",
      "Epoch 25/30\n",
      "100000/100000 [==============================] - 24s 239us/step - loss: 3.2984\n",
      "Epoch 26/30\n",
      "100000/100000 [==============================] - 24s 239us/step - loss: 3.2975\n",
      "Epoch 27/30\n",
      "100000/100000 [==============================] - 24s 239us/step - loss: 3.2843\n",
      "Epoch 28/30\n",
      "100000/100000 [==============================] - 24s 241us/step - loss: 3.2792\n",
      "Epoch 29/30\n",
      "100000/100000 [==============================] - 24s 242us/step - loss: 3.2742\n",
      "Epoch 30/30\n",
      "100000/100000 [==============================] - 24s 240us/step - loss: 3.2751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f27db921f50>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs = 30, verbose=1, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.51453488],\n",
       "        [0.63953488],\n",
       "        [0.60901163],\n",
       "        [0.33866279],\n",
       "        [0.71511628],\n",
       "        [0.6744186 ],\n",
       "        [0.25726744],\n",
       "        [0.6744186 ],\n",
       "        [0.81540698],\n",
       "        [0.65552326],\n",
       "        [0.60901163],\n",
       "        [0.69331395],\n",
       "        [0.34447674],\n",
       "        [0.42005814],\n",
       "        [0.2747093 ],\n",
       "        [0.42005814],\n",
       "        [0.40116279],\n",
       "        [0.37936047],\n",
       "        [0.54215116],\n",
       "        [0.33430233],\n",
       "        [0.3255814 ],\n",
       "        [0.43313953],\n",
       "        [0.51598837],\n",
       "        [0.45203488],\n",
       "        [0.90406977],\n",
       "        [0.29215116],\n",
       "        [0.90406977],\n",
       "        [0.43895349],\n",
       "        [0.33866279],\n",
       "        [0.49563953],\n",
       "        [0.33866279],\n",
       "        [0.51453488],\n",
       "        [0.63953488],\n",
       "        [0.60901163],\n",
       "        [0.69331395],\n",
       "        [0.61046512],\n",
       "        [0.65552326],\n",
       "        [0.81540698],\n",
       "        [0.65552326],\n",
       "        [0.75436047]]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(START[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sergey_pavlov/myEnv/lib/python2.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "START = list(X[40342][:,0])\n",
    "\n",
    "new_track = []\n",
    "previous_note = int(688 * START[-1])\n",
    "\n",
    "for i in range(25):\n",
    "    prediction = model.predict(np.reshape(START,(1,8,1)))\n",
    "    prediction[0,previous_note] = 0.0\n",
    "    p = take_one(prediction)\n",
    "    previous_note = p\n",
    "    new_track.append(int_to_notes[p])\n",
    "    START = START[1:]\n",
    "    START.append(p)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['61/64',\n",
       " '94/8',\n",
       " '42/4',\n",
       " '22/2',\n",
       " '49/4',\n",
       " '44/1',\n",
       " '52/4',\n",
       " '25/1',\n",
       " '79/4',\n",
       " '31/32',\n",
       " '66/16',\n",
       " '25/1',\n",
       " '90/8',\n",
       " '66/16',\n",
       " '90/8',\n",
       " '79/4',\n",
       " '66/16',\n",
       " '91/8',\n",
       " '94/4',\n",
       " '90/8',\n",
       " '26/1',\n",
       " '79/4',\n",
       " '43/1',\n",
       " '43/8',\n",
       " '35/4']"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(START.reshape(1,40,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "sum(pvals[:-1]) > 1.0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-dd5dcf0d51b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.multinomial\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: sum(pvals[:-1]) > 1.0"
     ]
    }
   ],
   "source": [
    "np.random.multinomial(1,list(prediction)[0], size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_one(pred):\n",
    "    a = np.log(pred[0]) / 1.0 \n",
    "    dist = np.exp(a)/np.sum(np.exp(a)) \n",
    "    choices = range(len(a))\n",
    "    return np.random.choice(choices, p=dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[43,\n",
       " 61,\n",
       " 346,\n",
       " 224,\n",
       " 554,\n",
       " 491,\n",
       " 231,\n",
       " 435,\n",
       " 356,\n",
       " 276,\n",
       " 434,\n",
       " 474,\n",
       " 557,\n",
       " 654,\n",
       " 474,\n",
       " 398,\n",
       " 448,\n",
       " 291,\n",
       " 233,\n",
       " 434,\n",
       " 242,\n",
       " 448,\n",
       " 382,\n",
       " 457,\n",
       " 339,\n",
       " 553,\n",
       " 375,\n",
       " 242,\n",
       " 464,\n",
       " 474,\n",
       " 85,\n",
       " 231,\n",
       " 448,\n",
       " 341,\n",
       " 448,\n",
       " 448,\n",
       " 291,\n",
       " 614,\n",
       " 584,\n",
       " 464,\n",
       " 474,\n",
       " 242,\n",
       " 568,\n",
       " 434,\n",
       " 85,\n",
       " 233,\n",
       " 352,\n",
       " 455,\n",
       " 500,\n",
       " 474]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[take_one(prediction)for i in range(50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.51453488],\n",
       "       [0.63953488],\n",
       "       [0.60901163],\n",
       "       [0.33866279],\n",
       "       [0.71511628],\n",
       "       [0.6744186 ],\n",
       "       [0.25726744],\n",
       "       [0.6744186 ],\n",
       "       [0.81540698],\n",
       "       [0.65552326],\n",
       "       [0.60901163],\n",
       "       [0.69331395],\n",
       "       [0.34447674],\n",
       "       [0.42005814],\n",
       "       [0.2747093 ],\n",
       "       [0.42005814],\n",
       "       [0.40116279],\n",
       "       [0.37936047],\n",
       "       [0.54215116],\n",
       "       [0.33430233],\n",
       "       [0.3255814 ],\n",
       "       [0.43313953],\n",
       "       [0.51598837],\n",
       "       [0.45203488],\n",
       "       [0.90406977],\n",
       "       [0.29215116],\n",
       "       [0.90406977],\n",
       "       [0.43895349],\n",
       "       [0.33866279],\n",
       "       [0.49563953],\n",
       "       [0.33866279],\n",
       "       [0.51453488],\n",
       "       [0.63953488],\n",
       "       [0.60901163],\n",
       "       [0.69331395],\n",
       "       [0.61046512],\n",
       "       [0.65552326],\n",
       "       [0.81540698],\n",
       "       [0.65552326],\n",
       "       [0.75436047]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "new_track = []\n",
    "\n",
    "for i in range(200):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_notes = {notes_to_int[x]:x for x in notes_to_int.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_note(n):\n",
    "    ind = n.find('/')\n",
    "    return (int(n[:ind]), int(n[ind+1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_note(\"100/100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_jazz = pickle.load( open( \"jazz.txt\", \"rb\" ) )\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "seq_length = 8\n",
    "\n",
    "for seq in data_jazz: \n",
    "    n_chars = len(seq)\n",
    "    for i in range(0, n_chars - seq_length, 1):\n",
    "        seq_in = seq[i:i + seq_length]\n",
    "        seq_out = seq[i + seq_length]\n",
    "        dataX.append(seq_in)\n",
    "        dataY.append(seq_out)\n",
    "n_patterns = len(dataX)\n",
    "\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:300000]\n",
    "Y = dataY[:300000]\n",
    "X = X / float(688)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(Y, 688)\n",
    "# define the LSTM model\n",
    "\n",
    "model_jazz = Sequential()\n",
    "model_jazz.add(LSTM(100, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model_jazz.add(Dropout(0.2))\n",
    "\n",
    "model_jazz.add(Flatten())\n",
    "model_jazz.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "opt = keras.optimizers.Adam()\n",
    "model_jazz.compile(loss='categorical_crossentropy', optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "300000/300000 [==============================] - 75s 249us/step - loss: 5.1251\n",
      "Epoch 2/60\n",
      "300000/300000 [==============================] - 73s 243us/step - loss: 5.0513\n",
      "Epoch 3/60\n",
      "300000/300000 [==============================] - 73s 242us/step - loss: 5.0028\n",
      "Epoch 4/60\n",
      "300000/300000 [==============================] - 73s 244us/step - loss: 4.9486\n",
      "Epoch 5/60\n",
      "300000/300000 [==============================] - 73s 242us/step - loss: 4.8872\n",
      "Epoch 6/60\n",
      "300000/300000 [==============================] - 75s 249us/step - loss: 4.8385\n",
      "Epoch 7/60\n",
      "300000/300000 [==============================] - 78s 260us/step - loss: 4.7982\n",
      "Epoch 8/60\n",
      "300000/300000 [==============================] - 77s 256us/step - loss: 4.7624\n",
      "Epoch 9/60\n",
      "300000/300000 [==============================] - 74s 247us/step - loss: 4.7311\n",
      "Epoch 10/60\n",
      "300000/300000 [==============================] - 77s 258us/step - loss: 4.7021\n",
      "Epoch 11/60\n",
      "300000/300000 [==============================] - 78s 261us/step - loss: 4.6773\n",
      "Epoch 12/60\n",
      "300000/300000 [==============================] - 78s 258us/step - loss: 4.6524\n",
      "Epoch 13/60\n",
      "300000/300000 [==============================] - 76s 254us/step - loss: 4.6320\n",
      "Epoch 14/60\n",
      "300000/300000 [==============================] - 79s 262us/step - loss: 4.6105\n",
      "Epoch 15/60\n",
      "300000/300000 [==============================] - 77s 258us/step - loss: 4.5936\n",
      "Epoch 16/60\n",
      "300000/300000 [==============================] - 77s 256us/step - loss: 4.5743\n",
      "Epoch 17/60\n",
      "300000/300000 [==============================] - 75s 249us/step - loss: 4.5583\n",
      "Epoch 18/60\n",
      "300000/300000 [==============================] - 76s 253us/step - loss: 4.5409\n",
      "Epoch 19/60\n",
      "300000/300000 [==============================] - 79s 264us/step - loss: 4.5247\n",
      "Epoch 20/60\n",
      "300000/300000 [==============================] - 76s 254us/step - loss: 4.5107\n",
      "Epoch 21/60\n",
      "300000/300000 [==============================] - 77s 257us/step - loss: 4.4960\n",
      "Epoch 22/60\n",
      "300000/300000 [==============================] - 80s 268us/step - loss: 4.4822\n",
      "Epoch 23/60\n",
      "300000/300000 [==============================] - 76s 252us/step - loss: 4.4708\n",
      "Epoch 24/60\n",
      "300000/300000 [==============================] - 74s 248us/step - loss: 4.4592\n",
      "Epoch 25/60\n",
      "300000/300000 [==============================] - 75s 250us/step - loss: 4.4472\n",
      "Epoch 26/60\n",
      "300000/300000 [==============================] - 76s 253us/step - loss: 4.4372\n",
      "Epoch 27/60\n",
      "300000/300000 [==============================] - 76s 254us/step - loss: 4.4264\n",
      "Epoch 28/60\n",
      "300000/300000 [==============================] - 78s 261us/step - loss: 4.4157\n",
      "Epoch 29/60\n",
      "300000/300000 [==============================] - 75s 251us/step - loss: 4.4068\n",
      "Epoch 30/60\n",
      "300000/300000 [==============================] - 77s 257us/step - loss: 4.3976\n",
      "Epoch 31/60\n",
      "300000/300000 [==============================] - 76s 254us/step - loss: 4.3896\n",
      "Epoch 32/60\n",
      "300000/300000 [==============================] - 76s 255us/step - loss: 4.3810\n",
      "Epoch 33/60\n",
      "300000/300000 [==============================] - 77s 256us/step - loss: 4.3727\n",
      "Epoch 34/60\n",
      "300000/300000 [==============================] - 74s 247us/step - loss: 4.3630\n",
      "Epoch 35/60\n",
      "300000/300000 [==============================] - 78s 261us/step - loss: 4.3580\n",
      "Epoch 36/60\n",
      "300000/300000 [==============================] - 78s 259us/step - loss: 4.3487\n",
      "Epoch 37/60\n",
      "300000/300000 [==============================] - 77s 256us/step - loss: 4.3424\n",
      "Epoch 38/60\n",
      "300000/300000 [==============================] - 76s 253us/step - loss: 4.3361\n",
      "Epoch 39/60\n",
      "300000/300000 [==============================] - 78s 259us/step - loss: 4.3314\n",
      "Epoch 40/60\n",
      "300000/300000 [==============================] - 77s 256us/step - loss: 4.3234\n",
      "Epoch 41/60\n",
      "300000/300000 [==============================] - 75s 250us/step - loss: 4.3171\n",
      "Epoch 42/60\n",
      "300000/300000 [==============================] - 74s 248us/step - loss: 4.3118\n",
      "Epoch 43/60\n",
      "300000/300000 [==============================] - 73s 243us/step - loss: 4.3045\n",
      "Epoch 44/60\n",
      "300000/300000 [==============================] - 77s 256us/step - loss: 4.3011\n",
      "Epoch 45/60\n",
      "300000/300000 [==============================] - 76s 252us/step - loss: 4.2956\n",
      "Epoch 46/60\n",
      "300000/300000 [==============================] - 77s 255us/step - loss: 4.2896\n",
      "Epoch 47/60\n",
      "300000/300000 [==============================] - 76s 252us/step - loss: 4.2846\n",
      "Epoch 48/60\n",
      "300000/300000 [==============================] - 76s 255us/step - loss: 4.2803\n",
      "Epoch 49/60\n",
      "300000/300000 [==============================] - 77s 257us/step - loss: 4.2770\n",
      "Epoch 50/60\n",
      "300000/300000 [==============================] - 76s 252us/step - loss: 4.2701\n",
      "Epoch 51/60\n",
      "300000/300000 [==============================] - 72s 239us/step - loss: 4.2647\n",
      "Epoch 52/60\n",
      "300000/300000 [==============================] - 74s 245us/step - loss: 4.2618\n",
      "Epoch 53/60\n",
      "300000/300000 [==============================] - 71s 237us/step - loss: 4.2581\n",
      "Epoch 54/60\n",
      "300000/300000 [==============================] - 74s 246us/step - loss: 4.2551\n",
      "Epoch 55/60\n",
      "300000/300000 [==============================] - 75s 249us/step - loss: 4.2512\n",
      "Epoch 56/60\n",
      "300000/300000 [==============================] - 77s 256us/step - loss: 4.2460\n",
      "Epoch 57/60\n",
      "300000/300000 [==============================] - 77s 258us/step - loss: 4.2434\n",
      "Epoch 58/60\n",
      "300000/300000 [==============================] - 77s 258us/step - loss: 4.2397\n",
      "Epoch 59/60\n",
      "300000/300000 [==============================] - 74s 247us/step - loss: 4.2355\n",
      "Epoch 60/60\n",
      "300000/300000 [==============================] - 75s 251us/step - loss: 4.2313\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f27da463050>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_jazz.fit(X, y, epochs = 60, verbose=1, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sergey_pavlov/myEnv/lib/python2.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "START = list(X[40342][:,0])\n",
    "\n",
    "new_track = []\n",
    "previous_note = int(688 * START[-1])\n",
    "\n",
    "for i in range(25):\n",
    "    prediction = model_jazz.predict(np.reshape(START,(1,8,1)))\n",
    "    prediction[0,previous_note] = 0.0\n",
    "    p = take_one(prediction)\n",
    "    previous_note = p\n",
    "    new_track.append(int_to_notes[p])\n",
    "    START = START[1:]\n",
    "    START.append(p)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['95/16',\n",
       " '41/8',\n",
       " '39/1',\n",
       " '78/16',\n",
       " '41/2',\n",
       " '48/16',\n",
       " '120/64',\n",
       " '89/64',\n",
       " '120/64',\n",
       " '34/64',\n",
       " '120/64',\n",
       " '41/2',\n",
       " '120/64',\n",
       " '89/64',\n",
       " '120/64',\n",
       " '41/2',\n",
       " '120/64',\n",
       " '88/4',\n",
       " '120/64',\n",
       " '78/16',\n",
       " '103/16',\n",
       " '120/64',\n",
       " '41/2',\n",
       " '120/64',\n",
       " '56/64']"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['91/2', '103/16', '105/4', '36/8', '64/2', '97/16', '57/4', '72/4']"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[int_to_notes[int(x*688)] for x in list(X[40342][:,0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_to_int = {'0/8': 268,\n",
    " '1/64': 27,\n",
    " '100/16': 456,\n",
    " '100/32': 605,\n",
    " '100/4': 178,\n",
    " '100/64': 371,\n",
    " '100/8': 114,\n",
    " '101/1': 60,\n",
    " '101/16': 422,\n",
    " '101/2': 62,\n",
    " '101/32': 154,\n",
    " '101/4': 66,\n",
    " '101/64': 250,\n",
    " '101/8': 109,\n",
    " '102/16': 444,\n",
    " '102/32': 152,\n",
    " '102/64': 20,\n",
    " '102/8': 133,\n",
    " '103/1': 117,\n",
    " '103/16': 455,\n",
    " '103/2': 102,\n",
    " '103/32': 30,\n",
    " '103/4': 115,\n",
    " '103/64': 466,\n",
    " '103/8': 129,\n",
    " '104/16': 283,\n",
    " '104/2': 493,\n",
    " '104/32': 175,\n",
    " '104/4': 139,\n",
    " '104/64': 70,\n",
    " '104/8': 137,\n",
    " '105/16': 405,\n",
    " '105/32': 147,\n",
    " '105/4': 132,\n",
    " '105/64': 390,\n",
    " '105/8': 264,\n",
    " '106/16': 582,\n",
    " '106/32': 49,\n",
    " '106/64': 413,\n",
    " '106/8': 145,\n",
    " '107/16': 408,\n",
    " '107/32': 56,\n",
    " '107/4': 146,\n",
    " '107/64': 134,\n",
    " '107/8': 141,\n",
    " '108/16': 427,\n",
    " '108/2': 51,\n",
    " '108/32': 160,\n",
    " '108/4': 52,\n",
    " '108/64': 533,\n",
    " '108/8': 407,\n",
    " '109/32': 179,\n",
    " '109/64': 263,\n",
    " '11/64': 391,\n",
    " '110/32': 10,\n",
    " '110/64': 562,\n",
    " '111/32': 384,\n",
    " '111/64': 63,\n",
    " '112/16': 521,\n",
    " '112/32': 246,\n",
    " '112/64': 499,\n",
    " '113/32': 265,\n",
    " '113/64': 173,\n",
    " '114/32': 303,\n",
    " '114/64': 554,\n",
    " '115/32': 324,\n",
    " '115/64': 232,\n",
    " '116/32': 423,\n",
    " '116/64': 128,\n",
    " '117/32': 412,\n",
    " '117/64': 167,\n",
    " '118/32': 308,\n",
    " '118/64': 383,\n",
    " '119/32': 302,\n",
    " '119/64': 372,\n",
    " '120/16': 306,\n",
    " '120/32': 584,\n",
    " '120/64': 488,\n",
    " '121/64': 288,\n",
    " '122/32': 45,\n",
    " '122/64': 424,\n",
    " '2/64': 429,\n",
    " '21/1': 336,\n",
    " '22/16': 254,\n",
    " '22/2': 352,\n",
    " '22/64': 271,\n",
    " '24/2': 344,\n",
    " '24/32': 595,\n",
    " '24/4': 298,\n",
    " '24/64': 248,\n",
    " '24/8': 432,\n",
    " '25/16': 338,\n",
    " '25/8': 345,\n",
    " '26/16': 370,\n",
    " '26/32': 100,\n",
    " '26/4': 340,\n",
    " '26/64': 184,\n",
    " '26/8': 334,\n",
    " '27/16': 360,\n",
    " '27/32': 127,\n",
    " '27/4': 335,\n",
    " '27/64': 307,\n",
    " '27/8': 337,\n",
    " '28/4': 418,\n",
    " '28/64': 74,\n",
    " '28/8': 101,\n",
    " '29/1': 414,\n",
    " '29/16': 349,\n",
    " '29/32': 506,\n",
    " '29/4': 416,\n",
    " '29/64': 327,\n",
    " '29/8': 417,\n",
    " '3/16': 577,\n",
    " '3/32': 221,\n",
    " '3/64': 135,\n",
    " '30/16': 64,\n",
    " '30/32': 509,\n",
    " '30/64': 176,\n",
    " '30/8': 573,\n",
    " '31/16': 156,\n",
    " '31/2': 157,\n",
    " '31/32': 497,\n",
    " '31/4': 159,\n",
    " '31/64': 602,\n",
    " '31/8': 130,\n",
    " '32/1': 198,\n",
    " '32/16': 354,\n",
    " '32/2': 195,\n",
    " '32/32': 22,\n",
    " '32/4': 194,\n",
    " '32/64': 527,\n",
    " '32/8': 191,\n",
    " '33/1': 79,\n",
    " '33/16': 26,\n",
    " '33/32': 44,\n",
    " '33/4': 578,\n",
    " '33/64': 355,\n",
    " '33/8': 82,\n",
    " '34/1': 163,\n",
    " '34/16': 229,\n",
    " '34/2': 164,\n",
    " '34/32': 580,\n",
    " '34/4': 162,\n",
    " '34/64': 478,\n",
    " '34/8': 161,\n",
    " '35/1': 190,\n",
    " '35/16': 305,\n",
    " '35/2': 192,\n",
    " '35/32': 269,\n",
    " '35/4': 189,\n",
    " '35/64': 276,\n",
    " '35/8': 196,\n",
    " '36/1': 430,\n",
    " '36/16': 217,\n",
    " '36/2': 165,\n",
    " '36/32': 438,\n",
    " '36/4': 168,\n",
    " '36/64': 200,\n",
    " '36/8': 170,\n",
    " '37/1': 451,\n",
    " '37/16': 187,\n",
    " '37/2': 169,\n",
    " '37/32': 453,\n",
    " '37/4': 172,\n",
    " '37/64': 557,\n",
    " '37/8': 166,\n",
    " '38/1': 601,\n",
    " '38/16': 222,\n",
    " '38/2': 85,\n",
    " '38/32': 603,\n",
    " '38/4': 87,\n",
    " '38/64': 318,\n",
    " '38/8': 90,\n",
    " '39/1': 5,\n",
    " '39/16': 228,\n",
    " '39/2': 93,\n",
    " '39/32': 587,\n",
    " '39/4': 95,\n",
    " '39/64': 50,\n",
    " '39/8': 84,\n",
    " '4/16': 576,\n",
    " '4/32': 299,\n",
    " '4/64': 588,\n",
    " '40/1': 566,\n",
    " '40/16': 174,\n",
    " '40/2': 568,\n",
    " '40/32': 519,\n",
    " '40/4': 411,\n",
    " '40/64': 150,\n",
    " '40/8': 571,\n",
    " '41/1': 529,\n",
    " '41/16': 98,\n",
    " '41/2': 530,\n",
    " '41/32': 426,\n",
    " '41/4': 528,\n",
    " '41/64': 339,\n",
    " '41/8': 526,\n",
    " '42/1': 183,\n",
    " '42/16': 53,\n",
    " '42/2': 535,\n",
    " '42/32': 322,\n",
    " '42/4': 538,\n",
    " '42/64': 421,\n",
    " '42/8': 532,\n",
    " '43/1': 177,\n",
    " '43/16': 73,\n",
    " '43/2': 531,\n",
    " '43/32': 319,\n",
    " '43/4': 534,\n",
    " '43/64': 59,\n",
    " '43/8': 536,\n",
    " '44/1': 500,\n",
    " '44/16': 25,\n",
    " '44/2': 523,\n",
    " '44/32': 364,\n",
    " '44/4': 492,\n",
    " '44/64': 457,\n",
    " '44/8': 494,\n",
    " '45/1': 220,\n",
    " '45/16': 559,\n",
    " '45/2': 440,\n",
    " '45/32': 537,\n",
    " '45/4': 441,\n",
    " '45/64': 42,\n",
    " '45/8': 433,\n",
    " '46/1': 436,\n",
    " '46/16': 508,\n",
    " '46/2': 435,\n",
    " '46/32': 158,\n",
    " '46/4': 434,\n",
    " '46/64': 410,\n",
    " '46/8': 442,\n",
    " '47/1': 572,\n",
    " '47/16': 219,\n",
    " '47/2': 570,\n",
    " '47/32': 498,\n",
    " '47/4': 569,\n",
    " '47/64': 397,\n",
    " '47/8': 357,\n",
    " '48/1': 402,\n",
    " '48/16': 31,\n",
    " '48/2': 464,\n",
    " '48/32': 385,\n",
    " '48/4': 462,\n",
    " '48/64': 17,\n",
    " '48/8': 460,\n",
    " '49/1': 395,\n",
    " '49/16': 396,\n",
    " '49/2': 461,\n",
    " '49/32': 404,\n",
    " '49/4': 459,\n",
    " '49/64': 300,\n",
    " '49/8': 463,\n",
    " '5/32': 369,\n",
    " '5/64': 253,\n",
    " '50/1': 400,\n",
    " '50/16': 567,\n",
    " '50/2': 274,\n",
    " '50/32': 182,\n",
    " '50/4': 277,\n",
    " '50/64': 553,\n",
    " '50/8': 279,\n",
    " '51/1': 420,\n",
    " '51/16': 543,\n",
    " '51/2': 278,\n",
    " '51/32': 185,\n",
    " '51/4': 280,\n",
    " '51/64': 293,\n",
    " '51/8': 275,\n",
    " '52/1': 272,\n",
    " '52/16': 585,\n",
    " '52/2': 273,\n",
    " '52/32': 301,\n",
    " '52/4': 270,\n",
    " '52/64': 210,\n",
    " '52/8': 267,\n",
    " '53/1': 310,\n",
    " '53/16': 36,\n",
    " '53/2': 312,\n",
    " '53/32': 388,\n",
    " '53/4': 309,\n",
    " '53/64': 19,\n",
    " '53/8': 315,\n",
    " '54/1': 316,\n",
    " '54/16': 76,\n",
    " '54/2': 314,\n",
    " '54/32': 366,\n",
    " '54/4': 313,\n",
    " '54/64': 255,\n",
    " '54/8': 311,\n",
    " '55/1': 21,\n",
    " '55/16': 377,\n",
    " '55/2': 202,\n",
    " '55/32': 28,\n",
    " '55/4': 201,\n",
    " '55/64': 266,\n",
    " '55/8': 208,\n",
    " '56/1': 205,\n",
    " '56/16': 415,\n",
    " '56/2': 206,\n",
    " '56/32': 151,\n",
    " '56/4': 207,\n",
    " '56/64': 522,\n",
    " '56/8': 29,\n",
    " '57/1': 233,\n",
    " '57/16': 504,\n",
    " '57/2': 235,\n",
    " '57/32': 180,\n",
    " '57/4': 236,\n",
    " '57/64': 325,\n",
    " '57/8': 238,\n",
    " '58/1': 239,\n",
    " '58/16': 558,\n",
    " '58/2': 237,\n",
    " '58/32': 199,\n",
    " '58/4': 68,\n",
    " '58/64': 581,\n",
    " '58/8': 234,\n",
    " '59/1': 227,\n",
    " '59/16': 512,\n",
    " '59/2': 226,\n",
    " '59/32': 171,\n",
    " '59/4': 213,\n",
    " '59/64': 260,\n",
    " '59/8': 231,\n",
    " '60/1': 552,\n",
    " '60/16': 8,\n",
    " '60/2': 551,\n",
    " '60/32': 261,\n",
    " '60/4': 550,\n",
    " '60/64': 514,\n",
    " '60/8': 244,\n",
    " '61/1': 549,\n",
    " '61/16': 23,\n",
    " '61/2': 548,\n",
    " '61/32': 560,\n",
    " '61/4': 547,\n",
    " '61/64': 188,\n",
    " '61/8': 247,\n",
    " '62/1': 373,\n",
    " '62/16': 43,\n",
    " '62/2': 544,\n",
    " '62/32': 394,\n",
    " '62/4': 546,\n",
    " '62/64': 490,\n",
    " '62/8': 204,\n",
    " '63/1': 539,\n",
    " '63/16': 32,\n",
    " '63/2': 540,\n",
    " '63/32': 381,\n",
    " '63/4': 541,\n",
    " '63/64': 138,\n",
    " '63/8': 542,\n",
    " '64/1': 13,\n",
    " '64/16': 149,\n",
    " '64/2': 12,\n",
    " '64/32': 419,\n",
    " '64/4': 15,\n",
    " '64/64': 520,\n",
    " '64/8': 16,\n",
    " '65/1': 593,\n",
    " '65/16': 67,\n",
    " '65/2': 592,\n",
    " '65/32': 333,\n",
    " '65/4': 594,\n",
    " '65/64': 99,\n",
    " '65/8': 589,\n",
    " '66/1': 46,\n",
    " '66/16': 487,\n",
    " '66/2': 517,\n",
    " '66/32': 212,\n",
    " '66/4': 515,\n",
    " '66/64': 450,\n",
    " '66/8': 33,\n",
    " '67/1': 513,\n",
    " '67/16': 505,\n",
    " '67/2': 38,\n",
    " '67/32': 240,\n",
    " '67/4': 511,\n",
    " '67/64': 142,\n",
    " '67/8': 516,\n",
    " '68/1': 598,\n",
    " '68/16': 590,\n",
    " '68/2': 597,\n",
    " '68/32': 243,\n",
    " '68/4': 596,\n",
    " '68/64': 496,\n",
    " '68/8': 599,\n",
    " '69/1': 4,\n",
    " '69/16': 48,\n",
    " '69/2': 3,\n",
    " '69/32': 326,\n",
    " '69/4': 2,\n",
    " '69/64': 225,\n",
    " '69/8': 1,\n",
    " '7/64': 575,\n",
    " '70/1': 387,\n",
    " '70/16': 374,\n",
    " '70/2': 256,\n",
    " '70/32': 103,\n",
    " '70/4': 252,\n",
    " '70/64': 9,\n",
    " '70/8': 398,\n",
    " '71/1': 258,\n",
    " '71/16': 343,\n",
    " '71/2': 259,\n",
    " '71/32': 71,\n",
    " '71/4': 257,\n",
    " '71/64': 320,\n",
    " '71/8': 350,\n",
    " '72/1': 331,\n",
    " '72/16': 561,\n",
    " '72/2': 330,\n",
    " '72/32': 209,\n",
    " '72/4': 332,\n",
    " '72/64': 586,\n",
    " '72/8': 328,\n",
    " '73/1': 376,\n",
    " '73/16': 18,\n",
    " '73/2': 375,\n",
    " '73/32': 574,\n",
    " '73/4': 378,\n",
    " '73/64': 389,\n",
    " '73/8': 379,\n",
    " '74/1': 555,\n",
    " '74/16': 518,\n",
    " '74/2': 282,\n",
    " '74/32': 242,\n",
    " '74/4': 284,\n",
    " '74/64': 6,\n",
    " '74/8': 285,\n",
    " '75/1': 564,\n",
    " '75/16': 524,\n",
    " '75/2': 286,\n",
    " '75/32': 251,\n",
    " '75/4': 287,\n",
    " '75/64': 346,\n",
    " '75/8': 281,\n",
    " '76/1': 292,\n",
    " '76/16': 502,\n",
    " '76/2': 291,\n",
    " '76/32': 153,\n",
    " '76/4': 290,\n",
    " '76/64': 55,\n",
    " '76/8': 604,\n",
    " '77/1': 296,\n",
    " '77/16': 479,\n",
    " '77/2': 295,\n",
    " '77/32': 140,\n",
    " '77/4': 294,\n",
    " '77/64': 382,\n",
    " '77/8': 65,\n",
    " '78/1': 367,\n",
    " '78/16': 525,\n",
    " '78/2': 368,\n",
    " '78/32': 245,\n",
    " '78/4': 365,\n",
    " '78/64': 155,\n",
    " '78/8': 363,\n",
    " '79/1': 356,\n",
    " '79/16': 491,\n",
    " '79/2': 358,\n",
    " '79/32': 215,\n",
    " '79/4': 353,\n",
    " '79/64': 454,\n",
    " '79/8': 361,\n",
    " '80/1': 75,\n",
    " '80/16': 431,\n",
    " '80/2': 77,\n",
    " '80/32': 97,\n",
    " '80/4': 78,\n",
    " '80/64': 193,\n",
    " '80/8': 80,\n",
    " '81/1': 91,\n",
    " '81/16': 401,\n",
    " '81/2': 92,\n",
    " '81/32': 144,\n",
    " '81/4': 94,\n",
    " '81/64': 510,\n",
    " '81/8': 83,\n",
    " '82/1': 89,\n",
    " '82/16': 317,\n",
    " '82/2': 88,\n",
    " '82/32': 591,\n",
    " '82/4': 86,\n",
    " '82/64': 223,\n",
    " '82/8': 96,\n",
    " '83/1': 113,\n",
    " '83/16': 399,\n",
    " '83/2': 111,\n",
    " '83/32': 54,\n",
    " '83/4': 110,\n",
    " '83/64': 579,\n",
    " '83/8': 107,\n",
    " '84/1': 106,\n",
    " '84/16': 347,\n",
    " '84/2': 108,\n",
    " '84/32': 72,\n",
    " '84/4': 104,\n",
    " '84/64': 323,\n",
    " '84/8': 112,\n",
    " '85/1': 119,\n",
    " '85/16': 393,\n",
    " '85/2': 120,\n",
    " '85/32': 143,\n",
    " '85/4': 118,\n",
    " '85/64': 24,\n",
    " '85/8': 116,\n",
    " '86/1': 148,\n",
    " '86/16': 351,\n",
    " '86/2': 124,\n",
    " '86/32': 14,\n",
    " '86/4': 126,\n",
    " '86/64': 61,\n",
    " '86/8': 122,\n",
    " '87/1': 136,\n",
    " '87/16': 329,\n",
    " '87/2': 121,\n",
    " '87/32': 7,\n",
    " '87/4': 123,\n",
    " '87/64': 362,\n",
    " '87/8': 125,\n",
    " '88/1': 289,\n",
    " '88/16': 392,\n",
    " '88/2': 40,\n",
    " '88/32': 131,\n",
    " '88/4': 41,\n",
    " '88/64': 214,\n",
    " '88/8': 34,\n",
    " '89/1': 563,\n",
    " '89/16': 386,\n",
    " '89/2': 35,\n",
    " '89/32': 105,\n",
    " '89/4': 37,\n",
    " '89/64': 476,\n",
    " '89/8': 39,\n",
    " '9/16': 0,\n",
    " '9/4': 428,\n",
    " '90/1': 473,\n",
    " '90/16': 197,\n",
    " '90/2': 471,\n",
    " '90/32': 545,\n",
    " '90/4': 470,\n",
    " '90/64': 425,\n",
    " '90/8': 468,\n",
    " '91/1': 447,\n",
    " '91/16': 181,\n",
    " '91/2': 446,\n",
    " '91/32': 437,\n",
    " '91/4': 445,\n",
    " '91/64': 81,\n",
    " '91/8': 452,\n",
    " '92/1': 448,\n",
    " '92/16': 262,\n",
    " '92/2': 449,\n",
    " '92/32': 11,\n",
    " '92/4': 342,\n",
    " '92/64': 380,\n",
    " '92/8': 443,\n",
    " '93/1': 297,\n",
    " '93/16': 241,\n",
    " '93/2': 321,\n",
    " '93/32': 583,\n",
    " '93/4': 304,\n",
    " '93/64': 58,\n",
    " '93/8': 439,\n",
    " '94/1': 474,\n",
    " '94/16': 203,\n",
    " '94/2': 482,\n",
    " '94/32': 477,\n",
    " '94/4': 484,\n",
    " '94/64': 224,\n",
    " '94/8': 486,\n",
    " '95/1': 495,\n",
    " '95/16': 218,\n",
    " '95/2': 485,\n",
    " '95/32': 507,\n",
    " '95/4': 489,\n",
    " '95/64': 556,\n",
    " '95/8': 483,\n",
    " '96/1': 458,\n",
    " '96/16': 249,\n",
    " '96/2': 480,\n",
    " '96/32': 600,\n",
    " '96/4': 359,\n",
    " '96/64': 503,\n",
    " '96/8': 475,\n",
    " '97/1': 467,\n",
    " '97/16': 216,\n",
    " '97/2': 469,\n",
    " '97/32': 341,\n",
    " '97/4': 465,\n",
    " '97/64': 186,\n",
    " '97/8': 472,\n",
    " '98/1': 565,\n",
    " '98/16': 230,\n",
    " '98/32': 501,\n",
    " '98/4': 406,\n",
    " '98/64': 403,\n",
    " '98/8': 57,\n",
    " '99/1': 348,\n",
    " '99/16': 211,\n",
    " '99/32': 481,\n",
    " '99/4': 409,\n",
    " '99/64': 69,\n",
    " '99/8': 47}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda2]",
   "language": "python",
   "name": "conda-env-Anaconda2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
